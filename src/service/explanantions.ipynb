{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b28d24de-a207-4a78-bbbc-77a34e30ccf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mongodb.py\n",
      "postgres.py\n",
      "sqlite.py\n",
      "__init__.py\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def list_folder_names(folder_path: str):\n",
    "    \"\"\"\n",
    "    Returns a list of all file and directory names in the given folder.\n",
    "    \n",
    "    Parameters:\n",
    "    folder_path (str): A raw string path to the folder (e.g., r\"C:\\path\\to\\folder\").\n",
    "    \n",
    "    Returns:\n",
    "    List[str]: A list of names (not full paths) of the folder's immediate contents.\n",
    "    \"\"\"\n",
    "    path = Path(folder_path)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Path '{folder_path}' does not exist.\")\n",
    "    return [item.name for item in path.iterdir()]\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "folder_path = r\"C:\\Users\\Ans\\Desktop\\code\\16_agent_services_toolkit\\agent-service-toolkit\\src\\memory\"\n",
    "names = list_folder_names(folder_path)\n",
    "\n",
    "for name in names:\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85188ed9-d645-4726-9073-6b2f371fb699",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50441029-4280-46a4-824c-a983c2191b80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c681be7c-1adb-4ed2-a83e-d45045d54d43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f20090a-3bfa-4166-9af1-1042d539e108",
   "metadata": {},
   "source": [
    "# Logging\n",
    "\n",
    "<img src=\"./images/levels.png\" alt=\"Levels diagram\" width=\"400\"/>\n",
    "\n",
    "\n",
    "### basics: https://www.youtube.com/watch?v=g8nQ90Hk328\n",
    "### medium: https://www.youtube.com/watch?v=A3FkYRN9qog\n",
    "### advanced: https://www.youtube.com/watch?v=9L77QExPmI0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6797336-3eed-4e3a-9fde-3508603ad94e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90247a18-48e2-4b9b-9ab3-2aaf0ee1faf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d868127a-b3db-4466-8ca9-a708c8bf4564",
   "metadata": {},
   "source": [
    "# core/settings.py\n",
    "\n",
    "## message:2 -> branch:6\n",
    "\n",
    "### https://chatgpt.com/share/68972751-0b64-8009-a9d0-596765902fad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7faf6ea4-0b8a-4913-9e08-2c405408eecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from enum import StrEnum, auto\n",
    "# from typing import TypeAlias\n",
    "\n",
    "\n",
    "# class Provider(StrEnum):\n",
    "#     OPENAI            = auto()\n",
    "#     OPENAI_COMPATIBLE = auto()\n",
    "#     AZURE_OPENAI      = auto()\n",
    "#     DEEPSEEK          = auto()\n",
    "#     ANTHROPIC         = auto()\n",
    "#     GOOGLE            = auto()\n",
    "#     VERTEXAI          = auto()\n",
    "#     GROQ              = auto()\n",
    "#     AWS               = auto()\n",
    "#     OLLAMA            = auto()\n",
    "#     OPENROUTER        = auto()\n",
    "#     FAKE              = auto()\n",
    "\n",
    "\n",
    "# class OpenAIModelName(StrEnum):\n",
    "#     \"\"\"https://platform.openai.com/docs/models/gpt-4o\"\"\"\n",
    "\n",
    "#     GPT_4O_MINI = \"gpt-4o-mini\"\n",
    "#     GPT_4O = \"gpt-4o\"\n",
    "\n",
    "\n",
    "# class AzureOpenAIModelName(StrEnum):\n",
    "#     \"\"\"Azure OpenAI model names\"\"\"\n",
    "\n",
    "#     AZURE_GPT_4O = \"azure-gpt-4o\"\n",
    "#     AZURE_GPT_4O_MINI = \"azure-gpt-4o-mini\"\n",
    "\n",
    "\n",
    "# class DeepseekModelName(StrEnum):\n",
    "#     \"\"\"https://api-docs.deepseek.com/quick_start/pricing\"\"\"\n",
    "\n",
    "#     DEEPSEEK_CHAT = \"deepseek-chat\"\n",
    "\n",
    "\n",
    "# class AnthropicModelName(StrEnum):\n",
    "#     \"\"\"https://docs.anthropic.com/en/docs/about-claude/models#model-names\"\"\"\n",
    "\n",
    "#     HAIKU_3 = \"claude-3-haiku\"\n",
    "#     HAIKU_35 = \"claude-3.5-haiku\"\n",
    "#     SONNET_35 = \"claude-3.5-sonnet\"\n",
    "\n",
    "\n",
    "# class GoogleModelName(StrEnum):\n",
    "#     \"\"\"https://ai.google.dev/gemini-api/docs/models/gemini\"\"\"\n",
    "\n",
    "#     GEMINI_15_PRO = \"gemini-1.5-pro\"\n",
    "#     GEMINI_20_FLASH = \"gemini-2.0-flash\"\n",
    "#     GEMINI_20_FLASH_LITE = \"gemini-2.0-flash-lite\"\n",
    "#     GEMINI_25_FLASH = \"gemini-2.5-flash\"\n",
    "#     GEMINI_25_PRO = \"gemini-2.5-pro\"\n",
    "\n",
    "\n",
    "# class VertexAIModelName(StrEnum):\n",
    "#     \"\"\"https://cloud.google.com/vertex-ai/generative-ai/docs/models\"\"\"\n",
    "\n",
    "#     GEMINI_15_PRO = \"gemini-1.5-pro\"\n",
    "#     GEMINI_20_FLASH = \"gemini-2.0-flash\"\n",
    "#     GEMINI_20_FLASH_LITE = \"models/gemini-2.0-flash-lite\"\n",
    "#     GEMINI_25_FLASH = \"models/gemini-2.5-flash\"\n",
    "#     GEMINI_25_PRO = \"gemini-2.5-pro\"\n",
    "\n",
    "\n",
    "# class GroqModelName(StrEnum):\n",
    "#     \"\"\"https://console.groq.com/docs/models\"\"\"\n",
    "\n",
    "#     LLAMA_31_8B = \"llama-3.1-8b\"\n",
    "#     LLAMA_33_70B = \"llama-3.3-70b\"\n",
    "\n",
    "#     LLAMA_GUARD_4_12B = \"meta-llama/llama-guard-4-12b\"\n",
    "\n",
    "\n",
    "# class AWSModelName(StrEnum):\n",
    "#     \"\"\"https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html\"\"\"\n",
    "\n",
    "#     BEDROCK_HAIKU = \"bedrock-3.5-haiku\"\n",
    "#     BEDROCK_SONNET = \"bedrock-3.5-sonnet\"\n",
    "\n",
    "\n",
    "# class OllamaModelName(StrEnum):\n",
    "#     \"\"\"https://ollama.com/search\"\"\"\n",
    "\n",
    "#     OLLAMA_GENERIC = \"ollama\"\n",
    "\n",
    "\n",
    "# class OpenRouterModelName(StrEnum):\n",
    "#     \"\"\"https://openrouter.ai/models\"\"\"\n",
    "\n",
    "#     GEMINI_25_FLASH = \"google/gemini-2.5-flash\"\n",
    "\n",
    "\n",
    "# class OpenAICompatibleName(StrEnum):\n",
    "#     \"\"\"https://platform.openai.com/docs/guides/text-generation\"\"\"\n",
    "\n",
    "#     OPENAI_COMPATIBLE = \"openai-compatible\"\n",
    "\n",
    "\n",
    "# class FakeModelName(StrEnum):\n",
    "#     \"\"\"Fake model for testing.\"\"\"\n",
    "\n",
    "#     FAKE = \"fake\"\n",
    "\n",
    "\n",
    "# AllModelEnum: TypeAlias = (\n",
    "#     OpenAIModelName\n",
    "#     | OpenAICompatibleName\n",
    "#     | AzureOpenAIModelName\n",
    "#     | DeepseekModelName\n",
    "#     | AnthropicModelName\n",
    "#     | GoogleModelName\n",
    "#     | VertexAIModelName\n",
    "#     | GroqModelName\n",
    "#     | AWSModelName\n",
    "#     | OllamaModelName\n",
    "#     | OpenRouterModelName\n",
    "#     | FakeModelName\n",
    "# )\n",
    "\n",
    "\n",
    "# from enum import StrEnum\n",
    "# from json import loads\n",
    "# from typing import Annotated, Any\n",
    "\n",
    "# from dotenv import find_dotenv\n",
    "# from pydantic import (\n",
    "#     BeforeValidator,\n",
    "#     Field,\n",
    "#     HttpUrl,\n",
    "#     SecretStr,\n",
    "#     TypeAdapter,\n",
    "#     computed_field,\n",
    "# )\n",
    "# from pydantic_settings import BaseSettings, SettingsConfigDict\n",
    "\n",
    "# from schema.models import (\n",
    "#     AllModelEnum,\n",
    "#     AnthropicModelName,\n",
    "#     AWSModelName,\n",
    "#     AzureOpenAIModelName,\n",
    "#     DeepseekModelName,\n",
    "#     FakeModelName,\n",
    "#     GoogleModelName,\n",
    "#     GroqModelName,\n",
    "#     OllamaModelName,\n",
    "#     OpenAICompatibleName,\n",
    "#     OpenAIModelName,\n",
    "#     OpenRouterModelName,\n",
    "#     Provider,\n",
    "#     VertexAIModelName,\n",
    "# )\n",
    "\n",
    "\n",
    "# class DatabaseType(StrEnum):\n",
    "#     SQLITE = \"sqlite\"\n",
    "#     POSTGRES = \"postgres\"\n",
    "#     MONGO = \"mongo\"\n",
    "\n",
    "\n",
    "# def check_str_is_http(x: str) -> str:\n",
    "#     http_url_adapter = TypeAdapter(HttpUrl)\n",
    "#     return str(http_url_adapter.validate_python(x))\n",
    "\n",
    "\n",
    "# class Settings(BaseSettings):\n",
    "#     model_config = SettingsConfigDict(\n",
    "#                     env_file          = find_dotenv(),\n",
    "#                     env_file_encoding = \"utf-8\",\n",
    "#                     env_ignore_empty  = True,\n",
    "#                     extra             = \"ignore\",\n",
    "#                     validate_default  = False,\n",
    "#     )\n",
    "#     MODE: str | None = None\n",
    "\n",
    "#     HOST: str = \"0.0.0.0\"\n",
    "#     PORT: int = 8080\n",
    "\n",
    "#     AUTH_SECRET: SecretStr | None = None\n",
    "\n",
    "#     OPENAI_API_KEY: SecretStr | None = None\n",
    "#     DEEPSEEK_API_KEY: SecretStr | None = None\n",
    "#     ANTHROPIC_API_KEY: SecretStr | None = None\n",
    "#     GOOGLE_API_KEY: SecretStr | None = None\n",
    "#     GOOGLE_APPLICATION_CREDENTIALS: SecretStr | None = None\n",
    "#     GROQ_API_KEY: SecretStr | None = None\n",
    "#     USE_AWS_BEDROCK: bool = False\n",
    "#     OLLAMA_MODEL: str | None = None\n",
    "#     OLLAMA_BASE_URL: str | None = None\n",
    "#     USE_FAKE_MODEL: bool = False\n",
    "#     OPENROUTER_API_KEY: str | None = None\n",
    "\n",
    "\n",
    "\n",
    "#     # If DEFAULT_MODEL is None, it will be set in model_post_init\n",
    "#     DEFAULT_MODEL: AllModelEnum | None = None  # type: ignore[assignment]\n",
    "#     # That set() there is just creating an empty set — and right now, it’s being typed as set[AllModelEnum] so that later you can fill it with enum members        # from     any of your model name enums.\n",
    "#     AVAILABLE_MODELS: set[AllModelEnum] = set()  # type: ignore[assignment]\n",
    "\n",
    "#     # Set openai compatible api, mainly used for proof of concept\n",
    "#     COMPATIBLE_MODEL: str | None = None\n",
    "#     COMPATIBLE_API_KEY: SecretStr | None = None\n",
    "#     COMPATIBLE_BASE_URL: str | None = None\n",
    "\n",
    "#     OPENWEATHERMAP_API_KEY: SecretStr | None = None\n",
    "\n",
    "#     LANGCHAIN_TRACING_V2: bool = False\n",
    "#     LANGCHAIN_PROJECT: str = \"default\"\n",
    "#     LANGCHAIN_ENDPOINT: Annotated[str, BeforeValidator(check_str_is_http)] = (\n",
    "#         \"https://api.smith.langchain.com\"\n",
    "#     )\n",
    "#     LANGCHAIN_API_KEY: SecretStr | None = None\n",
    "\n",
    "#     LANGFUSE_TRACING: bool = False\n",
    "#     LANGFUSE_HOST: Annotated[str, BeforeValidator(check_str_is_http)] = \"https://cloud.langfuse.com\"\n",
    "#     LANGFUSE_PUBLIC_KEY: SecretStr | None = None\n",
    "#     LANGFUSE_SECRET_KEY: SecretStr | None = None\n",
    "\n",
    "#     # Database Configuration\n",
    "#     DATABASE_TYPE: DatabaseType = (\n",
    "#         DatabaseType.SQLITE\n",
    "#     )  # Options: DatabaseType.SQLITE or DatabaseType.POSTGRES\n",
    "#     SQLITE_DB_PATH: str = \"checkpoints.db\"\n",
    "\n",
    "#     # PostgreSQL Configuration\n",
    "#     POSTGRES_USER: str | None = None\n",
    "#     POSTGRES_PASSWORD: SecretStr | None = None\n",
    "#     POSTGRES_HOST: str | None = None\n",
    "#     POSTGRES_PORT: int | None = None\n",
    "#     POSTGRES_DB: str | None = None\n",
    "\n",
    "#     # MongoDB Configuration\n",
    "#     MONGO_HOST: str | None = None\n",
    "#     MONGO_PORT: int | None = None\n",
    "#     MONGO_DB: str | None = None\n",
    "#     MONGO_USER: str | None = None\n",
    "#     MONGO_PASSWORD: SecretStr | None = None\n",
    "#     MONGO_AUTH_SOURCE: str | None = None\n",
    "\n",
    "#     # Azure OpenAI Settings\n",
    "#     AZURE_OPENAI_API_KEY: SecretStr | None = None\n",
    "#     AZURE_OPENAI_ENDPOINT: str | None = None\n",
    "#     AZURE_OPENAI_API_VERSION: str = \"2024-02-15-preview\"\n",
    "#     AZURE_OPENAI_DEPLOYMENT_MAP: dict[str, str] = Field(\n",
    "#         default_factory=dict, description=\"Map of model names to Azure deployment IDs\"\n",
    "#     )\n",
    "\n",
    "#     def model_post_init(self, __context: Any) -> None:\n",
    "#         api_keys = {\n",
    "#             Provider.OPENAI: self.OPENAI_API_KEY,\n",
    "#             Provider.OPENAI_COMPATIBLE: self.COMPATIBLE_BASE_URL and self.COMPATIBLE_MODEL,\n",
    "#             Provider.DEEPSEEK: self.DEEPSEEK_API_KEY,\n",
    "#             Provider.ANTHROPIC: self.ANTHROPIC_API_KEY,\n",
    "#             Provider.GOOGLE: self.GOOGLE_API_KEY,\n",
    "#             Provider.VERTEXAI: self.GOOGLE_APPLICATION_CREDENTIALS,\n",
    "#             Provider.GROQ: self.GROQ_API_KEY,\n",
    "#             Provider.AWS: self.USE_AWS_BEDROCK,\n",
    "#             Provider.OLLAMA: self.OLLAMA_MODEL,\n",
    "#             Provider.FAKE: self.USE_FAKE_MODEL,\n",
    "#             Provider.AZURE_OPENAI: self.AZURE_OPENAI_API_KEY,\n",
    "#             Provider.OPENROUTER: self.OPENROUTER_API_KEY,\n",
    "#         }\n",
    "#         active_keys = [k for k, v in api_keys.items() if v]\n",
    "#         if not active_keys:\n",
    "#             raise ValueError(\"At least one LLM API key must be provided.\")\n",
    "\n",
    "#         for provider in active_keys:\n",
    "#             match provider:\n",
    "#                 case Provider.OPENAI:\n",
    "#                     if self.DEFAULT_MODEL is None:\n",
    "#                         self.DEFAULT_MODEL = OpenAIModelName.GPT_4O_MINI\n",
    "#                     self.AVAILABLE_MODELS.update(set(OpenAIModelName))\n",
    "#                 case Provider.OPENAI_COMPATIBLE:\n",
    "#                     if self.DEFAULT_MODEL is None:\n",
    "#                         self.DEFAULT_MODEL = OpenAICompatibleName.OPENAI_COMPATIBLE\n",
    "#                     self.AVAILABLE_MODELS.update(set(OpenAICompatibleName))\n",
    "#                 case Provider.DEEPSEEK:\n",
    "#                     if self.DEFAULT_MODEL is None:\n",
    "#                         self.DEFAULT_MODEL = DeepseekModelName.DEEPSEEK_CHAT\n",
    "#                     self.AVAILABLE_MODELS.update(set(DeepseekModelName))\n",
    "#                 case Provider.ANTHROPIC:\n",
    "#                     if self.DEFAULT_MODEL is None:\n",
    "#                         self.DEFAULT_MODEL = AnthropicModelName.HAIKU_3\n",
    "#                     self.AVAILABLE_MODELS.update(set(AnthropicModelName))\n",
    "#                 case Provider.GOOGLE:\n",
    "#                     if self.DEFAULT_MODEL is None:\n",
    "#                         self.DEFAULT_MODEL = GoogleModelName.GEMINI_20_FLASH\n",
    "#                     self.AVAILABLE_MODELS.update(set(GoogleModelName))\n",
    "#                 case Provider.VERTEXAI:\n",
    "#                     if self.DEFAULT_MODEL is None:\n",
    "#                         self.DEFAULT_MODEL = VertexAIModelName.GEMINI_20_FLASH\n",
    "#                     self.AVAILABLE_MODELS.update(set(VertexAIModelName))\n",
    "#                 case Provider.GROQ:\n",
    "#                     if self.DEFAULT_MODEL is None:\n",
    "#                         self.DEFAULT_MODEL = GroqModelName.LLAMA_31_8B\n",
    "#                     self.AVAILABLE_MODELS.update(set(GroqModelName))\n",
    "#                 case Provider.AWS:\n",
    "#                     if self.DEFAULT_MODEL is None:\n",
    "#                         self.DEFAULT_MODEL = AWSModelName.BEDROCK_HAIKU\n",
    "#                     self.AVAILABLE_MODELS.update(set(AWSModelName))\n",
    "#                 case Provider.OLLAMA:\n",
    "#                     if self.DEFAULT_MODEL is None:\n",
    "#                         self.DEFAULT_MODEL = OllamaModelName.OLLAMA_GENERIC\n",
    "#                     self.AVAILABLE_MODELS.update(set(OllamaModelName))\n",
    "#                 case Provider.OPENROUTER:\n",
    "#                     if self.DEFAULT_MODEL is None:\n",
    "#                         self.DEFAULT_MODEL = OpenRouterModelName.GEMINI_25_FLASH\n",
    "#                     self.AVAILABLE_MODELS.update(set(OpenRouterModelName))\n",
    "#                 case Provider.FAKE:\n",
    "#                     if self.DEFAULT_MODEL is None:\n",
    "#                         self.DEFAULT_MODEL = FakeModelName.FAKE\n",
    "#                     self.AVAILABLE_MODELS.update(set(FakeModelName))\n",
    "#                 case Provider.AZURE_OPENAI:\n",
    "#                     if self.DEFAULT_MODEL is None:\n",
    "#                         self.DEFAULT_MODEL = AzureOpenAIModelName.AZURE_GPT_4O_MINI\n",
    "#                     self.AVAILABLE_MODELS.update(set(AzureOpenAIModelName))\n",
    "#                     # Validate Azure OpenAI settings if Azure provider is available\n",
    "#                     if not self.AZURE_OPENAI_API_KEY:\n",
    "#                         raise ValueError(\"AZURE_OPENAI_API_KEY must be set\")\n",
    "#                     if not self.AZURE_OPENAI_ENDPOINT:\n",
    "#                         raise ValueError(\"AZURE_OPENAI_ENDPOINT must be set\")\n",
    "#                     if not self.AZURE_OPENAI_DEPLOYMENT_MAP:\n",
    "#                         raise ValueError(\"AZURE_OPENAI_DEPLOYMENT_MAP must be set\")\n",
    "\n",
    "#                     # Parse deployment map if it's a string\n",
    "#                     if isinstance(self.AZURE_OPENAI_DEPLOYMENT_MAP, str):\n",
    "#                         try:\n",
    "#                             self.AZURE_OPENAI_DEPLOYMENT_MAP = loads(\n",
    "#                                 self.AZURE_OPENAI_DEPLOYMENT_MAP\n",
    "#                             )\n",
    "#                         except Exception as e:\n",
    "#                             raise ValueError(f\"Invalid AZURE_OPENAI_DEPLOYMENT_MAP JSON: {e}\")\n",
    "\n",
    "#                     # Validate required deployments exist\n",
    "#                     required_models = {\"gpt-4o\", \"gpt-4o-mini\"}\n",
    "#                     missing_models = required_models - set(self.AZURE_OPENAI_DEPLOYMENT_MAP.keys())\n",
    "#                     if missing_models:\n",
    "#                         raise ValueError(f\"Missing required Azure deployments: {missing_models}\")\n",
    "#                 case _:\n",
    "#                     raise ValueError(f\"Unknown provider: {provider}\")\n",
    "\n",
    "#     @computed_field  # type: ignore[prop-decorator]\n",
    "#     @property\n",
    "#     def BASE_URL(self) -> str:\n",
    "#         return f\"http://{self.HOST}:{self.PORT}\"\n",
    "\n",
    "#     def is_dev(self) -> bool:\n",
    "#         return self.MODE == \"dev\"\n",
    "\n",
    "\n",
    "# settings = Settings()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc194f6-7634-4a46-b8fd-ec6db0cb7ccc",
   "metadata": {},
   "source": [
    "alright — let’s unpack this “main block” (the settings module) three ways:\n",
    "\n",
    "1. the global objective in the context of the whole repo\n",
    "2. a sub-block by sub-block tour\n",
    "3. a line-by-line “what this does for the whole system”\n",
    "\n",
    "---\n",
    "\n",
    "# Global objective (in the whole codebase)\n",
    "\n",
    "This module is the **single source of truth for configuration**. It:\n",
    "\n",
    "* Loads env vars (from `.env` if present), validates them, and exposes them as a typed `Settings` object.\n",
    "* Decides which **LLM providers** are “active” (based on which API keys/flags you set).\n",
    "* Populates the **default model** and the **set of available models** the whole system will advertise via `/info`.\n",
    "* Validates provider-specific requirements (notably **Azure OpenAI** deployments).\n",
    "* Centralizes service, tracing, and DB config that other components read:\n",
    "\n",
    "  * `service.py` reads `AUTH_SECRET`, `/info` pulls `DEFAULT_MODEL` + `AVAILABLE_MODELS`, `/health` checks `LANGFUSE_*`, the lifespan/db layer uses DB settings, etc.\n",
    "\n",
    "Net effect: everything else (agents, FastAPI service, streaming, history) can rely on `settings = Settings()` to know **what providers/models exist and how to talk to them**.\n",
    "\n",
    "---\n",
    "\n",
    "# Sub-block by sub-block\n",
    "\n",
    "**Imports (Pydantic, settings, enums, etc.)**\n",
    "Bring in the types and enum definitions you showed in your context block (providers + model names). Pydantic is used to load/validate from env vars.\n",
    "\n",
    "**`DatabaseType` enum**\n",
    "Declares which short-/long-term memory backends are supported (`sqlite`, `postgres`, `mongo`). The **lifespan** code in `service.py` will initialize the appropriate checkpointer/store depending on these values.\n",
    "\n",
    "**`check_str_is_http`**\n",
    "Tiny validator that coerces + validates a URL string; used to ensure things like `LANGFUSE_HOST` and `LANGCHAIN_ENDPOINT` are valid HTTP(S) URLs.\n",
    "\n",
    "**`Settings` class**\n",
    "\n",
    "* `model_config` tells Pydantic to read `.env`, ignore unknown keys, etc.\n",
    "* Fields grouped by purpose:\n",
    "\n",
    "  * **Service/HTTP**: host/port/base URL, auth secret\n",
    "  * **Provider keys + flags**: OpenAI, Anthropic, Google, Vertex, Groq, AWS Bedrock, Ollama, OpenRouter, OpenAI-compatible, Fake\n",
    "  * **Model selection**: `DEFAULT_MODEL`, `AVAILABLE_MODELS`\n",
    "  * **Tracing/analytics**: LangChain, Langfuse\n",
    "  * **DB choices**: SQLite/Postgres/Mongo, plus per-DB connection bits\n",
    "  * **Azure OpenAI** specifics (endpoint, version, deployment map)\n",
    "* **`model_post_init`**: after envs load, it:\n",
    "\n",
    "  * Ensures **at least one provider** is configured.\n",
    "  * Picks a **default model** (once) based on the **first active provider** encountered.\n",
    "  * Populates `AVAILABLE_MODELS` with all enums for each active provider.\n",
    "  * Validates **Azure OpenAI** (keys, endpoint, deployment map) and parses its map if passed as JSON.\n",
    "* **`BASE_URL` computed field** builds `http://{HOST}:{PORT}`.\n",
    "* **`is_dev`** toggles dev behavior based on `MODE`.\n",
    "\n",
    "**Module-level `settings = Settings()`**\n",
    "Eagerly loads and validates configuration at import time so the rest of the app can just use `from core.settings import settings`.\n",
    "\n",
    "---\n",
    "\n",
    "# Line-by-line (global objective of each line)\n",
    "\n",
    "I’ll keep each line crisp—what it contributes to the whole system.\n",
    "\n",
    "### Imports\n",
    "\n",
    "* `from enum import StrEnum` — Use string-valued enums for stable, readable env/model names system-wide.\n",
    "* `from json import loads` — Parse JSON strings from env (e.g., Azure deployment map).\n",
    "* `from typing import Annotated, Any` — Type annotations; `Annotated` attaches validators.\n",
    "* `from dotenv import find_dotenv` — Auto-locate a `.env` file so the service runs easily locally.\n",
    "* `from pydantic import ...` — Pydantic tools to validate/coerce env vars into safe settings objects.\n",
    "* `from pydantic_settings import BaseSettings, SettingsConfigDict` — Base class to read envs + config for how to read `.env`.\n",
    "* `from schema.models import (...)` — Pull provider + model enums to (a) select defaults, (b) advertise `/info`, (c) constrain settings.\n",
    "\n",
    "### `DatabaseType` enum\n",
    "\n",
    "* `class DatabaseType(StrEnum):` — Named DB choices the memory layer can switch on.\n",
    "* `SQLITE = \"sqlite\"`, `POSTGRES = \"postgres\"`, `MONGO = \"mongo\"` — Canonical strings that the lifespan store/checkpointer code will interpret.\n",
    "\n",
    "### `check_str_is_http`\n",
    "\n",
    "* `def check_str_is_http(x: str) -> str:` — Helper validator used by Annotated fields.\n",
    "* `http_url_adapter = TypeAdapter(HttpUrl)` — Prepare a Pydantic URL adapter.\n",
    "* `return str(http_url_adapter.validate_python(x))` — Validate & normalize a URL string so downstream components don’t get bad URLs.\n",
    "\n",
    "### `class Settings(BaseSettings):`\n",
    "\n",
    "* `model_config = SettingsConfigDict(...)` — Tell Pydantic where/how to load env:\n",
    "\n",
    "  * `env_file = find_dotenv()` — Read `.env` if present for local/dev.\n",
    "  * `env_file_encoding = \"utf-8\"` — Encoding for `.env`.\n",
    "  * `env_ignore_empty = True` — Empty env vars are treated as missing (prevents false “configured” state).\n",
    "  * `extra = \"ignore\"` — Unknown env vars are ignored (keeps deploys resilient).\n",
    "  * `validate_default = False` — Don’t validate defaults at class definition time.\n",
    "* `MODE: str | None = None` — Optional env to flip dev/other modes (`is_dev()` checks it).\n",
    "* `HOST: str = \"0.0.0.0\"` — Bind address for FastAPI server (used in `BASE_URL`).\n",
    "* `PORT: int = 8080` — Port for FastAPI server (used in `BASE_URL`).\n",
    "* `AUTH_SECRET: SecretStr | None = None` — If set, enables **Bearer auth** in `service.py` (router dependency).\n",
    "* Provider creds/flags (all optional, turn on a provider if present):\n",
    "\n",
    "  * `OPENAI_API_KEY: SecretStr | None = None` — Enables OpenAI provider.\n",
    "  * `DEEPSEEK_API_KEY: SecretStr | None = None` — Enables DeepSeek.\n",
    "  * `ANTHROPIC_API_KEY: SecretStr | None = None` — Enables Anthropic.\n",
    "  * `GOOGLE_API_KEY: SecretStr | None = None` — Enables Gemini via Google.\n",
    "  * `GOOGLE_APPLICATION_CREDENTIALS: SecretStr | None = None` — Enables Vertex AI auth.\n",
    "  * `GROQ_API_KEY: SecretStr | None = None` — Enables Groq.\n",
    "  * `USE_AWS_BEDROCK: bool = False` — Enables Bedrock when `True`.\n",
    "  * `OLLAMA_MODEL: str | None = None` — Enables Ollama if model name provided.\n",
    "  * `OLLAMA_BASE_URL: str | None = None` — Optional base URL for local Ollama server.\n",
    "  * `USE_FAKE_MODEL: bool = False` — Enables Fake model for tests/demos.\n",
    "  * `OPENROUTER_API_KEY: str | None = None` — Enables OpenRouter.\n",
    "* Model selection fields:\n",
    "\n",
    "  * `DEFAULT_MODEL: AllModelEnum | None = None` — Placeholder; set later in `model_post_init` if missing.\n",
    "  * `AVAILABLE_MODELS: set[AllModelEnum] = set()` — Accumulates every model name across active providers; used by `/info`.\n",
    "* OpenAI-compatible “shim” (for custom or self-hosted OpenAI-style servers):\n",
    "\n",
    "  * `COMPATIBLE_MODEL`, `COMPATIBLE_API_KEY`, `COMPATIBLE_BASE_URL` — Turn on a generic OpenAI-compatible backend if present.\n",
    "* Other API:\n",
    "\n",
    "  * `OPENWEATHERMAP_API_KEY` — Used by tools that call weather (not by the service core).\n",
    "* LangChain/LangSmith tracing:\n",
    "\n",
    "  * `LANGCHAIN_TRACING_V2: bool = False` — Opt-in to v2 tracing.\n",
    "  * `LANGCHAIN_PROJECT: str = \"default\"` — Project name for traces.\n",
    "  * `LANGCHAIN_ENDPOINT: Annotated[str, BeforeValidator(check_str_is_http)] = \"https://api.smith.langchain.com\"` — Validated URL for tracing API.\n",
    "  * `LANGCHAIN_API_KEY: SecretStr | None = None` — Auth for traces.\n",
    "* Langfuse tracing:\n",
    "\n",
    "  * `LANGFUSE_TRACING: bool = False` — Toggle Langfuse. `service.py:/health` uses this to check connectivity.\n",
    "  * `LANGFUSE_HOST: Annotated[str, BeforeValidator(check_str_is_http)] = \"https://cloud.langfuse.com\"` — Validated host.\n",
    "  * `LANGFUSE_PUBLIC_KEY`, `LANGFUSE_SECRET_KEY` — Auth; used when the service emits traces.\n",
    "* Database config:\n",
    "\n",
    "  * `DATABASE_TYPE: DatabaseType = DatabaseType.SQLITE` — Default to SQLite; lifespan uses this to pick checkpointer/store.\n",
    "  * `SQLITE_DB_PATH: str = \"checkpoints.db\"` — Default SQLite file for short-term memory.\n",
    "* Postgres:\n",
    "\n",
    "  * `POSTGRES_*` — Connection params used when `DATABASE_TYPE=postgres`.\n",
    "* Mongo:\n",
    "\n",
    "  * `MONGO_*` — Connection params used when `DATABASE_TYPE=mongo`.\n",
    "* Azure OpenAI:\n",
    "\n",
    "  * `AZURE_OPENAI_API_KEY`, `AZURE_OPENAI_ENDPOINT`, `AZURE_OPENAI_API_VERSION` — Core connection info.\n",
    "  * `AZURE_OPENAI_DEPLOYMENT_MAP: dict[str, str]` — Map **model name → deployment ID**; required for routing OpenAI model names to Azure deployments.\n",
    "\n",
    "### `def model_post_init(self, __context: Any) -> None:`\n",
    "\n",
    "Runs **after** Pydantic loads env vars. This is where the “active providers → default/available models” logic happens.\n",
    "\n",
    "* `api_keys = { ... }` — Build a dict of **provider → “truthy if configured”**. It uses:\n",
    "\n",
    "  * API keys when applicable,\n",
    "  * flags (`USE_AWS_BEDROCK`, `USE_FAKE_MODEL`),\n",
    "  * presence of values (`COMPATIBLE_BASE_URL and COMPATIBLE_MODEL`, `OLLAMA_MODEL`).\n",
    "* `active_keys = [k for k, v in api_keys.items() if v]` — Compute the list of **configured providers**.\n",
    "* `if not active_keys: raise ...` — Hard fail: the whole service needs at least one LLM to function.\n",
    "* `for provider in active_keys:` — For each configured provider:\n",
    "\n",
    "  * `match provider:` — Choose branch per provider.\n",
    "  * Each branch does two things:\n",
    "\n",
    "    1. **Set `DEFAULT_MODEL` if still unset** (first active provider wins):\n",
    "\n",
    "       * e.g., OpenAI → `GPT_4O_MINI`, Anthropic → `HAIKU_3`, Groq → `LLAMA_31_8B`, etc.\n",
    "    2. **Populate `AVAILABLE_MODELS`** with **the entire enum** for that provider so `/info` can advertise selectable models.\n",
    "* Special Azure branch:\n",
    "\n",
    "  * Sets default to `AzureOpenAIModelName.AZURE_GPT_4O_MINI`.\n",
    "  * Adds all Azure model enum members to `AVAILABLE_MODELS`.\n",
    "  * **Validates** Azure envs (key, endpoint, deployment map must exist).\n",
    "  * If `AZURE_OPENAI_DEPLOYMENT_MAP` is a string, `loads` parses it as JSON (supports `AZURE_OPENAI_DEPLOYMENT_MAP='{\"gpt-4o\":\"dep1\", ...}'` in `.env`).\n",
    "  * Ensures deployments for **`gpt-4o` and `gpt-4o-mini`** exist (these two models are required by the toolkit’s Azure path).\n",
    "* `case _:` — Guard against unknown providers (defensive programming).\n",
    "\n",
    "### Computed/base helpers\n",
    "\n",
    "* `@computed_field` + `def BASE_URL(self) -> str:` — Derive `http://{HOST}:{PORT}` so **clients** (and examples) can target the local server without recomputing.\n",
    "* `def is_dev(self) -> bool:` — Convenience for feature toggles in dev (`MODE==\"dev\"`).\n",
    "\n",
    "### Module instantiation\n",
    "\n",
    "* `settings = Settings()` — **Load and validate** configuration **eagerly** at import time. From here on, every module can `from core.settings import settings` and trust it’s valid.\n",
    "\n",
    "---\n",
    "\n",
    "## Why this matters to `service.py` specifically\n",
    "\n",
    "* **Auth**: `verify_bearer` reads `settings.AUTH_SECRET` to protect routes.\n",
    "* **/info**: Uses `settings.AVAILABLE_MODELS` and `settings.DEFAULT_MODEL` to advertise models per your env.\n",
    "* **Health**: `/health` checks `settings.LANGFUSE_TRACING` and keys to report tracing connectivity.\n",
    "* **Lifespan / memory**: DB-related fields (e.g., `DATABASE_TYPE`, `POSTGRES_*`) drive which checkpointer/store `initialize_database()` / `initialize_store()` choose.\n",
    "\n",
    "If you want, I can next trace a concrete path: “set these envs → see what `AVAILABLE_MODELS` becomes → what `/info` returns → how an agent invocation uses `DEFAULT_MODEL`” — just say the provider(s) you care about.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc8756f-d059-4a62-a24d-a43b610d4985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94511a23-aad5-48fb-b0a9-51e1107339c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6e6904a-dda9-4661-992a-0b890dd49e45",
   "metadata": {},
   "source": [
    "# How imports are working"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46ea7f1-1a8f-47b2-9627-108cc94c0c33",
   "metadata": {},
   "source": [
    "Here’s the summary in 10–15 lines:\n",
    "\n",
    "1. Your project uses a **`src/` layout**, where `service` and `memory` are **sibling packages** under `src`.\n",
    "2. Both folders have an `__init__.py`, making them **top-level Python packages**.\n",
    "3. Python searches packages using `sys.path`; when you run from the repo root with `src` on the path, it can see `memory`.\n",
    "4. **Absolute import** `from memory import …` works because Python finds `src/memory/__init__.py`.\n",
    "5. **Relative import** like `from .memory import …` would fail because there is no `service/memory` subfolder.\n",
    "6. `initialize_database()` and `initialize_store()` are exposed by `memory/__init__.py`.\n",
    "7. They **decide at runtime** which backend to use: SQLite (default), Postgres, or Mongo.\n",
    "8. These functions return **async context managers** for short-term (checkpointer) and long-term (store) memory.\n",
    "9. `service.py` can stay backend-agnostic because it just calls these two initializers.\n",
    "10. FastAPI’s `lifespan` uses them to set up memory for all agents on app startup.\n",
    "11. Python will resolve the import as long as `src` is on `sys.path` (via `-m`, `PYTHONPATH`, or editable install).\n",
    "12. Sibling packages under the same path can always import each other via **absolute imports**.\n",
    "13. That’s why `memory` doesn’t need to be inside the `service` folder.\n",
    "14. The current structure is correct and Pythonic for multi-package projects.\n",
    "15. Relative imports are only needed if the module is a **subfolder** of the importing package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37e2fdc-74d5-480a-8ae0-09a8eba098bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c39f66a4-06ae-47b0-8141-20320180b571",
   "metadata": {},
   "source": [
    "## **How to make changes in modules and make them globally available**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8085c4f8-fa2f-4353-93f0-3dbe2f5bf7be",
   "metadata": {},
   "source": [
    "Here’s how you would **add a simple function** in the `memory` module to print `\"server starting\"` and make it importable cleanly from `memory` in your `service.py` (or anywhere else in the project):\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Add the function**\n",
    "\n",
    "Open `C:\\Users\\Ans\\Desktop\\code\\16_agent_services_toolkit\\agent-service-toolkit\\src\\memory\\__init__.py` and add:\n",
    "\n",
    "```python\n",
    "def announce_start():\n",
    "    \"\"\"Print a message indicating the server is starting.\"\"\"\n",
    "    print(\"server starting\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Update `__all__`**\n",
    "\n",
    "`__all__` defines which symbols are exposed when you do:\n",
    "\n",
    "```python\n",
    "from memory import *\n",
    "```\n",
    "\n",
    "Currently it’s:\n",
    "\n",
    "```python\n",
    "__all__ = [\"initialize_database\", \"initialize_store\"]\n",
    "```\n",
    "\n",
    "Update to include your new function:\n",
    "\n",
    "```python\n",
    "__all__ = [\"initialize_database\", \"initialize_store\", \"announce_start\"]\n",
    "```\n",
    "\n",
    "This is **optional** unless you rely on `*` imports, but it’s a good practice for clarity.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Import and use in `service.py`**\n",
    "\n",
    "At the top of `service.py`:\n",
    "\n",
    "```python\n",
    "from memory import initialize_database, initialize_store, announce_start\n",
    "```\n",
    "\n",
    "Then, wherever you want to announce (e.g., in your `lifespan` startup before `yield`):\n",
    "\n",
    "```python\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI) -> AsyncGenerator[None, None]:\n",
    "    try:\n",
    "        announce_start()  # <-- This will print \"server starting\"\n",
    "        async with initialize_database() as saver, initialize_store() as store:\n",
    "            ...\n",
    "            yield\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during database/store initialization: {e}\")\n",
    "        raise\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Now when the server starts\n",
    "\n",
    "* FastAPI triggers `lifespan` → `announce_start()` runs\n",
    "* You’ll see:\n",
    "\n",
    "```\n",
    "server starting\n",
    "```\n",
    "\n",
    "in your console.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Verify import works**\n",
    "\n",
    "You can test in a Python shell (from `src` directory):\n",
    "\n",
    "`poetry env activate`\n",
    "\n",
    "copy the string for activating virtual env, then ->\n",
    "\n",
    "\n",
    "```bash\n",
    "python\n",
    ">>> from memory import announce_start\n",
    ">>> announce_start()\n",
    "server starting\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can also **show a clean pattern** for “startup hooks” so you don’t just `print` but also use **FastAPI’s logging** correctly for better production output.\n",
    "\n",
    "Do you want that? It’s a small change that makes it look professional.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19008590-65b3-4489-9cf9-c67935967f5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df70dc57-1209-4e9e-96ff-10c5c4d7e39d",
   "metadata": {},
   "source": [
    "# Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e3d336-9ae9-465d-a3ca-51d63ac80ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # `verify_bearer` is a **FastAPI dependency** that enforces optional Bearer token authentication:\n",
    "\n",
    "# # 1. Uses `HTTPBearer(auto_error=False)` to parse `Authorization: Bearer <token>` headers.\n",
    "# # 2. If no header is present, `http_auth` is `None`.\n",
    "# # 3. If `settings.AUTH_SECRET` is **unset**, auth is skipped.\n",
    "# # 4. If set, retrieves the real secret via `.get_secret_value()`.\n",
    "# # 5. Compares provided token (`http_auth.credentials`) to the secret.\n",
    "# # 6. If missing or incorrect → raises `HTTPException(401)`.\n",
    "# # 7. Returning `None` means the request is allowed.\n",
    "# # 8. Applied at the router level → all endpoints in that router are protected.\n",
    "# # 9. Provides simple, single-secret, service-wide authentication.\n",
    "# # 10. Acts as a **gatekeeper** without modifying the request object.\n",
    "\n",
    "# def verify_bearer(\n",
    "#     http_auth: Annotated[\n",
    "#         HTTPAuthorizationCredentials | None,\n",
    "#         Depends(HTTPBearer(description=\"Please provide AUTH_SECRET api key.\", auto_error=False)),\n",
    "#     ],\n",
    "# ) -> None:\n",
    "#     if not settings.AUTH_SECRET:\n",
    "#         return\n",
    "#     auth_secret = settings.AUTH_SECRET.get_secret_value()\n",
    "#     if not http_auth or http_auth.credentials != auth_secret:\n",
    "#         raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadf999a-443f-4ee5-bc7f-383d2bf0d0dd",
   "metadata": {},
   "source": [
    "This block is the **authentication dependency** for the FastAPI routes.\n",
    "Here’s what it does step by step:\n",
    "\n",
    "---\n",
    "\n",
    "### **Function signature**\n",
    "\n",
    "```python\n",
    "def verify_bearer(\n",
    "    http_auth: Annotated[\n",
    "        HTTPAuthorizationCredentials | None,\n",
    "        Depends(HTTPBearer(description=\"Please provide AUTH_SECRET api key.\", auto_error=False)),\n",
    "    ],\n",
    ") -> None:\n",
    "```\n",
    "\n",
    "* **`http_auth`**\n",
    "\n",
    "  * Annotated type:\n",
    "\n",
    "    ```python\n",
    "    HTTPAuthorizationCredentials | None\n",
    "    ```\n",
    "\n",
    "    * If the client provides a Bearer token, FastAPI parses it into an `HTTPAuthorizationCredentials` object.\n",
    "    * If no token is provided, it may be `None` because `auto_error=False`.\n",
    "\n",
    "  * Dependency:\n",
    "\n",
    "    ```python\n",
    "    Depends(HTTPBearer(...))\n",
    "    ```\n",
    "\n",
    "    * `HTTPBearer` is a FastAPI security dependency that:\n",
    "\n",
    "      1. Looks for an `Authorization` header like:\n",
    "\n",
    "         ```\n",
    "         Authorization: Bearer <token>\n",
    "         ```\n",
    "      2. Extracts the `<token>` as `http_auth.credentials`.\n",
    "      3. If `auto_error=False` → it **won’t raise automatically** if no header is present; instead, it returns `None`.\n",
    "\n",
    "* **Return type: `None`**\n",
    "\n",
    "  * This dependency either **returns successfully** (allowing the request to proceed) or **raises an HTTPException** (stopping the request).\n",
    "\n",
    "---\n",
    "\n",
    "### **Function logic**\n",
    "\n",
    "```python\n",
    "if not settings.AUTH_SECRET:\n",
    "    return\n",
    "```\n",
    "\n",
    "* If no `AUTH_SECRET` is set in your settings, the function returns immediately → **auth is disabled**.\n",
    "\n",
    "```python\n",
    "auth_secret = settings.AUTH_SECRET.get_secret_value()\n",
    "```\n",
    "\n",
    "* Retrieves the **actual secret string** from a Pydantic `SecretStr` or similar secret type.\n",
    "\n",
    "```python\n",
    "if not http_auth or http_auth.credentials != auth_secret:\n",
    "    raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED)\n",
    "```\n",
    "\n",
    "* If:\n",
    "\n",
    "  1. No `Authorization` header was provided (`http_auth is None`), **or**\n",
    "  2. The provided token doesn’t match the secret,\n",
    "\n",
    "  → raise **401 Unauthorized**.\n",
    "\n",
    "---\n",
    "\n",
    "### **How it is used**\n",
    "\n",
    "In your script:\n",
    "\n",
    "```python\n",
    "router = APIRouter(dependencies=[Depends(verify_bearer)])\n",
    "```\n",
    "\n",
    "* All routes in this `router` automatically require passing `verify_bearer`.\n",
    "* If the function raises `HTTPException`, FastAPI immediately stops and returns 401.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example behavior**\n",
    "\n",
    "1. **Auth disabled (`AUTH_SECRET=None`)**\n",
    "\n",
    "   * Any request passes, even without a token.\n",
    "\n",
    "2. **Auth enabled**\n",
    "\n",
    "   ```http\n",
    "   Authorization: Bearer correct_secret\n",
    "   ```\n",
    "\n",
    "   ✅ Request passes\n",
    "\n",
    "   ```http\n",
    "   Authorization: Bearer wrong_secret\n",
    "   ```\n",
    "\n",
    "   ❌ Returns 401\n",
    "\n",
    "   No header:\n",
    "   ❌ Returns 401\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can also **show the exact shape** of `http_auth` and explain why `Annotated` is used here instead of a plain type.\n",
    "\n",
    "Do you want that?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3922cf3b-e54c-4265-9cf9-ad2161972e5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f8a579-5637-4c7c-9e93-46833c3c5d2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a7d2e4c-6e6c-4342-ba4f-77838fb68f58",
   "metadata": {},
   "source": [
    "# lifespan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f618bed-95ef-4214-91de-2b14d81a2f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @asynccontextmanager\n",
    "# async def lifespan(app: FastAPI) -> AsyncGenerator[None, None]:\n",
    "#     \"\"\"\n",
    "#     Configurable lifespan that initializes the appropriate database checkpointer and store\n",
    "#     based on settings.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # Initialize both checkpointer (for short-term memory) and store (for long-term memory)\n",
    "#         async with initialize_database() as saver, initialize_store() as store:\n",
    "#             # Set up both components\n",
    "#             if hasattr(saver, \"setup\"):  # ignore: union-attr\n",
    "#                 await saver.setup()\n",
    "#             # Only setup store for Postgres as InMemoryStore doesn't need setup\n",
    "#             if hasattr(store, \"setup\"):  # ignore: union-attr\n",
    "#                 await store.setup()\n",
    "\n",
    "#             # Configure agents with both memory components\n",
    "#             agents = get_all_agent_info()\n",
    "#             for a in agents:\n",
    "#                 agent = get_agent(a.key)\n",
    "#                 # Set checkpointer for thread-scoped memory (conversation history)\n",
    "#                 agent.checkpointer = saver\n",
    "#                 # Set store for long-term memory (cross-conversation knowledge)\n",
    "#                 agent.store = store\n",
    "#             yield\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Error during database/store initialization: {e}\")\n",
    "#         raise\n",
    "\n",
    "\n",
    "# app = FastAPI(lifespan=lifespan)\n",
    "# router = APIRouter(dependencies=[Depends(verify_bearer)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5be046-7215-4b37-a1ee-a2a6395c64d2",
   "metadata": {},
   "source": [
    "               FastAPI App Startup\n",
    "                       │\n",
    "                       ▼\n",
    "        ┌─────────────────────────────────┐\n",
    "        │  asynccontextmanager: lifespan   │\n",
    "        └─────────────────────────────────┘\n",
    "                       │\n",
    "                       │ 1️⃣ Enter context\n",
    "                       │\n",
    "                       ▼\n",
    "        ┌─────────────────────────────────┐\n",
    "        │ Initialize short-term memory     │\n",
    "        │  saver = initialize_database()   │\n",
    "        │--------------------------------- │\n",
    "        │ Initialize long-term memory      │\n",
    "        │  store = initialize_store()      │\n",
    "        │--------------------------------- │\n",
    "        │ Optional setup: saver.setup()    │\n",
    "        │ Optional setup: store.setup()    │\n",
    "        │--------------------------------- │\n",
    "        │ Attach memory to all agents:     │\n",
    "        │   agent.checkpointer = saver     │\n",
    "        │   agent.store = store            │\n",
    "        └─────────────────────────────────┘\n",
    "                       │\n",
    "                       │ 2️⃣ yield  (hand control to FastAPI)\n",
    "                       ▼\n",
    "               FastAPI Starts Serving\n",
    "                       │\n",
    "                       │\n",
    "                       ▼\n",
    "       ┌─────────────────────────────────┐\n",
    "       │         Handle Requests          │\n",
    "       │     /invoke, /stream, /info ...  │\n",
    "       └─────────────────────────────────┘\n",
    "                       │\n",
    "                       │\n",
    "                       ▼\n",
    "              FastAPI Shutdown Event\n",
    "                       │\n",
    "                       │ 3️⃣ Exit context\n",
    "                       ▼\n",
    "        ┌─────────────────────────────────┐\n",
    "        │     (Teardown would happen)      │\n",
    "        │    e.g., close DB connections    │\n",
    "        │          release memory          │\n",
    "        └─────────────────────────────────┘\n",
    "                       │\n",
    "                       ▼\n",
    "                 Application Stops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a16bcba-04f0-4fff-95b8-a2dd28ebe4b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68e19d53-6bfa-491f-bb17-449ed828cddd",
   "metadata": {},
   "source": [
    "Thanks for sharing the `agents.py` context — this explains how `agents` are managed and why the `lifespan` block behaves the way it does. Let me explain **why older agents might appear to be replaced** and how that block interacts with your `agents` dictionary.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. What your `agents` structure looks like**\n",
    "\n",
    "```python\n",
    "agents: dict[str, Agent] = {\n",
    "    \"chatbot\": Agent(..., graph=chatbot),\n",
    "    \"research-assistant\": Agent(..., graph=research_assistant),\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "* Keys: `agent_id` strings\n",
    "* Values: `Agent` dataclass instances with:\n",
    "\n",
    "  * `description: str`\n",
    "  * `graph: AgentGraph` (either `CompiledStateGraph` or `Pregel`)\n",
    "\n",
    "---\n",
    "\n",
    "### **2. `get_agent()` and `get_all_agent_info()`**\n",
    "\n",
    "* **`get_agent(agent_id)`** → returns **`Agent.graph`**, not the `Agent` object itself.\n",
    "\n",
    "  * E.g., returns a `CompiledStateGraph` instance like `chatbot` or `research_assistant`.\n",
    "* **`get_all_agent_info()`** → returns lightweight `AgentInfo` dataclasses for API use:\n",
    "\n",
    "  ```python\n",
    "  [\n",
    "      AgentInfo(key=\"chatbot\", description=\"A simple chatbot.\"),\n",
    "      AgentInfo(key=\"research-assistant\", description=\"A research assistant with web search...\")\n",
    "      ...\n",
    "  ]\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. What `lifespan` is doing to agents**\n",
    "\n",
    "In your startup:\n",
    "\n",
    "```python\n",
    "agents = get_all_agent_info()\n",
    "for a in agents:\n",
    "    agent = get_agent(a.key)\n",
    "    agent.checkpointer = saver\n",
    "    agent.store = store\n",
    "```\n",
    "\n",
    "Step by step:\n",
    "\n",
    "1. `get_all_agent_info()` → returns **metadata objects**, not the agents themselves.\n",
    "2. Loop through them, then `get_agent(a.key)` → fetches the **actual `graph` instance**.\n",
    "3. Adds two attributes **at runtime** to that instance:\n",
    "\n",
    "   * `checkpointer` (short-term memory)\n",
    "   * `store` (long-term memory)\n",
    "\n",
    "**Effect:**\n",
    "Each **LangGraph agent** instance is dynamically augmented with these memory components at startup.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Why “older agents are replaced” feeling happens**\n",
    "\n",
    "A few key points:\n",
    "\n",
    "* **Your `agents` dict is global and static.**\n",
    "  When the service starts, these are the *only* agent objects in memory.\n",
    "\n",
    "* **`lifespan` modifies the same instances in place.**\n",
    "  They are not replaced; they are **mutated** to attach memory:\n",
    "\n",
    "  ```python\n",
    "  agent.checkpointer = saver\n",
    "  agent.store = store\n",
    "  ```\n",
    "\n",
    "* **No new instances are created.**\n",
    "  This means the same `chatbot` or `research_assistant` object is reused across requests.\n",
    "\n",
    "* **If you hot-reload or restart**, the entire dictionary is rebuilt from scratch because Python reloads the module.\n",
    "  That might feel like “old agents are replaced by new ones,” but it’s just module re-import + new memory objects each time.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Important subtlety**\n",
    "\n",
    "Your `agents` dictionary holds **graph objects** imported from submodules:\n",
    "\n",
    "```python\n",
    "from agents.chatbot import chatbot  # this is likely a CompiledStateGraph instance\n",
    "```\n",
    "\n",
    "When `lifespan` does:\n",
    "\n",
    "```python\n",
    "agent = get_agent(a.key)  # returns that same imported object\n",
    "agent.checkpointer = saver\n",
    "agent.store = store\n",
    "```\n",
    "\n",
    "* These attributes (`checkpointer`, `store`) **didn’t exist originally**.\n",
    "* Python is dynamic, so it attaches them to the object at runtime.\n",
    "* If your LangGraph agent is a `CompiledStateGraph` or `Pregel` instance, this is legal but **non-persistent**.\n",
    "* On restart → they’re “gone” until `lifespan` runs again.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Why you might feel they are replaced**\n",
    "\n",
    "* **Restart or reload** → new `agents` dictionary created.\n",
    "* **Memory objects (`saver` / `store`) are new each startup.**\n",
    "* **Dynamic attributes** don’t persist anywhere (not saved to DB).\n",
    "\n",
    "So if your app is **auto-reloading** (e.g., `uvicorn --reload`), every reload rebuilds the entire `agents` dictionary and reattaches memory objects.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Visualization**\n",
    "\n",
    "Here’s what happens on startup:\n",
    "\n",
    "```\n",
    "1. Import agents.py\n",
    "   agents = {\n",
    "      \"chatbot\": Agent(graph=<chatbot CompiledStateGraph>),\n",
    "      ...\n",
    "   }\n",
    "\n",
    "2. FastAPI lifespan runs:\n",
    "   saver, store = initialize_database(), initialize_store()\n",
    "   for each agent:\n",
    "       agent.graph.checkpointer = saver\n",
    "       agent.graph.store = store\n",
    "\n",
    "3. Requests start coming in:\n",
    "   get_agent(\"chatbot\") -> returns graph with memory attached\n",
    "```\n",
    "\n",
    "On reload, step 1 runs again → “old” objects are discarded → “new” ones are built.\n",
    "\n",
    "---\n",
    "\n",
    "If your concern is **persisting memory or state across restarts**, you’d need to either:\n",
    "\n",
    "1. Serialize state to the database/store (which seems to be the intended approach using `saver` + `store`), or\n",
    "2. Maintain agent instances externally (not reloaded each time).\n",
    "\n",
    "---\n",
    "\n",
    "I can also show you a **safe way to inspect and confirm** whether your agents are being replaced at runtime by adding a diagnostic log.\n",
    "\n",
    "Do you want me to show that next? It will help you confirm if agents are *mutated in place* or *recreated*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4571311-4294-47a0-bad6-7b8ae2866b99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be855407-5291-4872-b412-3fac130aacc6",
   "metadata": {},
   "source": [
    "# Async postgres store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4b6451-ef3d-4c0a-b6c2-6869eda1a75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# osi trah hi use hota hay, (a)sync support bhi hay!\n",
    "\n",
    "from langgraph.store.postgres import AsyncPostgresStore\n",
    "\n",
    "conn_string = \"postgresql://postgres:1538879@localhost:5430/long_term_memory\"\n",
    "\n",
    "async with AsyncPostgresStore.from_conn_string(conn_string) as store:\n",
    "    # Run migrations (creates tables & indexes). Only needed once.\n",
    "    await store.setup()\n",
    "    # ... use the store for put/get/search operations\n",
    "    # Put\n",
    "    await store.aput((\"users\",\"123\"), \"prefs\", {\"theme\":\"dark\"})\n",
    "    \n",
    "    # Get\n",
    "    prefs = await store.aget((\"users\",\"123\"), \"prefs\")\n",
    "    print(prefs)  # {\"theme\":\"dark\"}\n",
    "    \n",
    "    # # Delete\n",
    "    # await store.adelete((\"users\",\"123\"), \"prefs\")\n",
    "    \n",
    "    # Search textual matches\n",
    "    matches = await store.asearch((\"docs\",), query=\"guide\", limit=5)\n",
    "    \n",
    "    # # List sub-namespaces\n",
    "    # namespaces = await store.alist_namespaces((\"docs\",), max_depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5641eb44-ce2b-48fd-bd51-a4769c76b597",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8d8fc5c-66b3-432a-b48f-cc07908c429f",
   "metadata": {},
   "source": [
    "# @router.get(\"/info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb77360-0fb2-4613-91c3-a3a6d24d248f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b98eebb-d566-4737-859f-88e7f6fd7313",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fde52b6-f303-4e19-a40e-e43364cf64c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3cf295-0a5e-4dd1-9ee6-4a6f463bee81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38de875c-4dd7-4fd8-88b7-4313b569ce8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3da640ed-9dc5-43cb-97d6-6fed25a19575",
   "metadata": {},
   "source": [
    "## 🧠 What You're Looking At\n",
    "\n",
    "You're looking at the **schema models and enums** that define the response for the `GET /info` endpoint.\n",
    "\n",
    "These models do two key things:\n",
    "\n",
    "1. Structure the **JSON response** returned by FastAPI.\n",
    "2. Automatically generate documentation (via OpenAPI/Swagger) for API consumers.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 Let's Connect It Back to This:\n",
    "\n",
    "```python\n",
    "@router.get(\"/info\")\n",
    "async def info() -> ServiceMetadata:\n",
    "    models = list(settings.AVAILABLE_MODELS)\n",
    "    models.sort()\n",
    "    return ServiceMetadata(\n",
    "        agents=get_all_agent_info(),\n",
    "        models=models,\n",
    "        default_agent=DEFAULT_AGENT,\n",
    "        default_model=settings.DEFAULT_MODEL,\n",
    "    )\n",
    "```\n",
    "\n",
    "It returns an instance of `ServiceMetadata`. So let’s unpack that model in full context.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 1. `ServiceMetadata` (Pydantic Response Model)\n",
    "\n",
    "```python\n",
    "class ServiceMetadata(BaseModel):\n",
    "    \"\"\"Metadata about the service including available agents and models.\"\"\"\n",
    "\n",
    "    agents: list[AgentInfo]\n",
    "    models: list[AllModelEnum]\n",
    "    default_agent: str\n",
    "    default_model: AllModelEnum\n",
    "```\n",
    "\n",
    "This defines the **full JSON structure** returned to the client.\n",
    "\n",
    "### ➕ Field-by-field:\n",
    "\n",
    "| Field           | Type                 | Description                                       |\n",
    "| --------------- | -------------------- | ------------------------------------------------- |\n",
    "| `agents`        | `list[AgentInfo]`    | All agents available in the service.              |\n",
    "| `models`        | `list[AllModelEnum]` | All LLM model names this service supports.        |\n",
    "| `default_agent` | `str`                | The fallback agent used when one isn't specified. |\n",
    "| `default_model` | `AllModelEnum`       | The fallback LLM model name.                      |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 2. `AgentInfo`\n",
    "\n",
    "```python\n",
    "class AgentInfo(BaseModel):\n",
    "    \"\"\"Info about an available agent.\"\"\"\n",
    "\n",
    "    key: str\n",
    "    description: str\n",
    "```\n",
    "\n",
    "This is the shape of each item in the `agents` list.\n",
    "\n",
    "### Example:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"key\": \"research-assistant\",\n",
    "  \"description\": \"A research assistant for generating research papers.\"\n",
    "}\n",
    "```\n",
    "\n",
    "Where does this come from?\n",
    "\n",
    "* Likely from `get_all_agent_info()`, which returns a list of `AgentInfo` instances — one per agent.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 3. `AllModelEnum` — (a Type Alias for *many* model types)\n",
    "\n",
    "This is a **type union** alias (not a class):\n",
    "\n",
    "```python\n",
    "AllModelEnum: TypeAlias = (\n",
    "    OpenAIModelName\n",
    "    | OpenAICompatibleName\n",
    "    | AzureOpenAIModelName\n",
    "    | DeepseekModelName\n",
    "    | AnthropicModelName\n",
    "    | GoogleModelName\n",
    "    | VertexAIModelName\n",
    "    | GroqModelName\n",
    "    | AWSModelName\n",
    "    | OllamaModelName\n",
    "    | OpenRouterModelName\n",
    "    | FakeModelName\n",
    ")\n",
    "```\n",
    "\n",
    "It combines many individual enum classes — one per provider or platform.\n",
    "\n",
    "This means any string returned as a model must match one of the valid values from one of these enums.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 4. Model Enums\n",
    "\n",
    "Let’s look at a couple of them:\n",
    "\n",
    "### ✅ `OpenAIModelName`\n",
    "\n",
    "```python\n",
    "class OpenAIModelName(StrEnum):\n",
    "    GPT_4O_MINI = \"gpt-4o-mini\"\n",
    "    GPT_4O = \"gpt-4o\"\n",
    "```\n",
    "\n",
    "Valid model values from OpenAI. The keys are internal names, the values are what get serialized to JSON.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ `AzureOpenAIModelName`\n",
    "\n",
    "```python\n",
    "class AzureOpenAIModelName(StrEnum):\n",
    "    AZURE_GPT_4O = \"azure-gpt-4o\"\n",
    "    AZURE_GPT_4O_MINI = \"azure-gpt-4o-mini\"\n",
    "```\n",
    "\n",
    "Same thing, for Azure-hosted OpenAI models.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ `AnthropicModelName`\n",
    "\n",
    "```python\n",
    "class AnthropicModelName(StrEnum):\n",
    "    HAIKU_3 = \"claude-3-haiku\"\n",
    "    HAIKU_35 = \"claude-3.5-haiku\"\n",
    "    SONNET_35 = \"claude-3.5-sonnet\"\n",
    "```\n",
    "\n",
    "Models from Anthropic (Claude 3 family).\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 5. `AVAILABLE_MODELS`\n",
    "\n",
    "```python\n",
    "AVAILABLE_MODELS: set[AllModelEnum] = set()\n",
    "```\n",
    "\n",
    "* This is a `set` containing all supported models.\n",
    "* It gets converted to a list in `GET /info`:\n",
    "\n",
    "  ```python\n",
    "  models = list(settings.AVAILABLE_MODELS)\n",
    "  models.sort()\n",
    "  ```\n",
    "\n",
    "This is how you define **which specific models are enabled** in this deployment.\n",
    "\n",
    "> 🔍 Somewhere in your code or config (not shown here), you must be populating `AVAILABLE_MODELS` like:\n",
    ">\n",
    "> ```python\n",
    "> AVAILABLE_MODELS = {\n",
    ">     OpenAIModelName.GPT_4O,\n",
    ">     AnthropicModelName.HAIKU_3,\n",
    ">     DeepseekModelName.DEEPSEEK_CHAT,\n",
    "> }\n",
    "> ```\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## ✅ Why This Design Is Good\n",
    "\n",
    "* **Strong typing with enums**: prevents typos or unsupported models from sneaking into API responses.\n",
    "* **Clear OpenAPI docs**: Pydantic models generate schema docs, so developers using your API know what agents and models they can pick from.\n",
    "* **Flexible & extensible**: You can add new providers (e.g., `Cohere`, `Mistral`) by just extending `AllModelEnum`.\n",
    "\n",
    "---\n",
    "\n",
    "## 👇 In Summary\n",
    "\n",
    "| Thing                            | Purpose                                         |\n",
    "| -------------------------------- | ----------------------------------------------- |\n",
    "| `ServiceMetadata`                | Pydantic model defining the `/info` response    |\n",
    "| `AgentInfo`                      | Describes a single agent (key + description)    |\n",
    "| `AllModelEnum`                   | Union of all possible model enums               |\n",
    "| `*ModelName` enums               | Valid model names from providers                |\n",
    "| `AVAILABLE_MODELS`               | Set of currently active models in this app      |\n",
    "| `default_agent`, `default_model` | Fallbacks used when not specified by the client |\n",
    "\n",
    "---\n",
    "\n",
    "If you'd like to explore how `get_all_agent_info()` works next — or see how these enums get populated at runtime — let me know and I’ll trace it with you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01608ccc-7aee-42ad-ae31-94ec3c35a9d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5087e567-4fd9-44a3-ad80-0caab6bc802e",
   "metadata": {},
   "source": [
    "An **Enum** (short for *enumeration*) in Python is a special class that defines a **fixed set of named values**.\n",
    "It’s useful when you want to represent a set of constants that are **meaningful** and **type-safe**, instead of using raw strings or numbers everywhere.\n",
    "\n",
    "---\n",
    "\n",
    "## 1️⃣ Basic idea\n",
    "\n",
    "Without enums, you might do this:\n",
    "\n",
    "```python\n",
    "# Bad practice: loose strings everywhere\n",
    "status = \"success\"\n",
    "if status == \"success\":\n",
    "    ...\n",
    "```\n",
    "\n",
    "The risk? You could mistype `\"succes\"` and Python wouldn’t complain — it’s just a string.\n",
    "\n",
    "With an Enum:\n",
    "\n",
    "```python\n",
    "from enum import Enum\n",
    "\n",
    "class Status(Enum):\n",
    "    SUCCESS = \"success\"\n",
    "    ERROR = \"error\"\n",
    "    PENDING = \"pending\"\n",
    "\n",
    "status = Status.SUCCESS\n",
    "\n",
    "if status == Status.SUCCESS:\n",
    "    ...\n",
    "```\n",
    "\n",
    "Now:\n",
    "\n",
    "* Values are **namespaced** (you can’t accidentally mix them with random strings)\n",
    "* Typos are caught early by type checkers\n",
    "* Autocompletion in your IDE helps you choose valid options\n",
    "\n",
    "---\n",
    "\n",
    "## 2️⃣ How they work\n",
    "\n",
    "Each Enum member:\n",
    "\n",
    "* Has a **name** (e.g., `SUCCESS`)\n",
    "* Has a **value** (e.g., `\"success\"`)\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "print(Status.SUCCESS.name)   # SUCCESS\n",
    "print(Status.SUCCESS.value)  # success\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3️⃣ `StrEnum` vs normal `Enum`\n",
    "\n",
    "In your code, you’re using:\n",
    "\n",
    "```python\n",
    "from enum import StrEnum\n",
    "```\n",
    "\n",
    "This is like `Enum`, but each member automatically behaves like a **string**.\n",
    "That means:\n",
    "\n",
    "```python\n",
    "class Color(StrEnum):\n",
    "    RED = \"red\"\n",
    "\n",
    "print(Color.RED)        # red\n",
    "print(isinstance(Color.RED, str))  # True\n",
    "```\n",
    "\n",
    "This is handy when your values need to be sent as plain strings to APIs (like `\"gpt-4o\"`), but you still want enum safety in your code.\n",
    "\n",
    "---\n",
    "\n",
    "## 4️⃣ Why use them in your snippet?\n",
    "\n",
    "* **Prevent invalid values**: You can’t set a model to `\"gpt4\"` if only `\"gpt-4o\"` is valid.\n",
    "* **Make code self-documenting**: `OpenAIModelName.GPT_4O` is clearer than `\"gpt-4o\"`.\n",
    "* **IDE autocompletion**: Suggests only valid models/providers.\n",
    "* **Easy refactoring**: Change the value in one place, update everywhere.\n",
    "\n",
    "---\n",
    "\n",
    "In short:\n",
    "An **Enum** is Python’s way of saying,\n",
    "*\"Here’s a list of allowed options. You can only pick from these, and each one has a clear name and value.\"*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff630f4-5b15-4760-8e0e-e935a97cb718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "572baf4b-e1e3-4a60-a8f7-a3e8772430a7",
   "metadata": {},
   "source": [
    "What you’ve got here is essentially a **typed, enum-based catalog** of all the LLM providers and their model names, wrapped up so that they can be stored in a `ServiceMetadata` Pydantic model for use in code that selects or configures an AI service.\n",
    "\n",
    "Let me walk you through it step-by-step.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. **The `Provider` enum**\n",
    "\n",
    "```python\n",
    "class Provider(StrEnum):\n",
    "    OPENAI            = auto()\n",
    "    OPENAI_COMPATIBLE = auto()\n",
    "    AZURE_OPENAI      = auto()\n",
    "    ...\n",
    "```\n",
    "\n",
    "* **`StrEnum`** is like a regular Python `Enum`, but its members are also strings.\n",
    "* **`auto()`** automatically assigns values, here meaning each enum member’s value will be the same as its name (e.g., `\"OPENAI\"`).\n",
    "* This enum lists **service providers** — companies or APIs that serve LLMs.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Model name enums**\n",
    "\n",
    "Each provider has its own `StrEnum` of **model identifiers**, e.g.:\n",
    "\n",
    "```python\n",
    "class OpenAIModelName(StrEnum):\n",
    "    GPT_4O_MINI = \"gpt-4o-mini\"\n",
    "    GPT_4O = \"gpt-4o\"\n",
    "```\n",
    "\n",
    "These:\n",
    "\n",
    "* Use **string values** exactly as they are expected in API calls.\n",
    "* Are grouped by provider for clarity.\n",
    "* Include docstrings with links to official documentation.\n",
    "\n",
    "So you have:\n",
    "\n",
    "* `OpenAIModelName` for OpenAI\n",
    "* `AzureOpenAIModelName` for Azure’s OpenAI service\n",
    "* `DeepseekModelName` for Deepseek\n",
    "* `AnthropicModelName` for Claude models\n",
    "* … and so on.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **`AllModelEnum` type alias**\n",
    "\n",
    "```python\n",
    "AllModelEnum: TypeAlias = (\n",
    "    OpenAIModelName\n",
    "    | OpenAICompatibleName\n",
    "    | AzureOpenAIModelName\n",
    "    | DeepseekModelName\n",
    "    | AnthropicModelName\n",
    "    | GoogleModelName\n",
    "    | VertexAIModelName\n",
    "    | GroqModelName\n",
    "    | AWSModelName\n",
    "    | OllamaModelName\n",
    "    | OpenRouterModelName\n",
    "    | FakeModelName\n",
    ")\n",
    "```\n",
    "\n",
    "This means:\n",
    "\n",
    "> \"`AllModelEnum` can be **any one of these enums**.\"\n",
    "\n",
    "It lets you type things like:\n",
    "\n",
    "```python\n",
    "def use_model(model: AllModelEnum):\n",
    "    ...\n",
    "```\n",
    "\n",
    "and still get **type checking & autocompletion** for all model names, even though they come from different providers.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **`ServiceMetadata` model**\n",
    "\n",
    "```python\n",
    "class ServiceMetadata(BaseModel):\n",
    "    agents: list[AgentInfo]\n",
    "    models: list[AllModelEnum]\n",
    "    default_agent: str\n",
    "    default_model: AllModelEnum\n",
    "```\n",
    "\n",
    "This is a **Pydantic model** that:\n",
    "\n",
    "* Stores:\n",
    "\n",
    "  * Which agents are available (`agents`)\n",
    "  * Which models are available (`models`)\n",
    "  * Defaults for both (`default_agent`, `default_model`)\n",
    "* Uses your `AllModelEnum` type to ensure the model names are **valid**.\n",
    "* Adds `description` and `examples` metadata for API docs or schema generation.\n",
    "\n",
    "---\n",
    "\n",
    "### **In short**\n",
    "\n",
    "* **Enums** define *who* (providers) and *what* (model names) in a structured, type-safe way.\n",
    "* **`AllModelEnum`** is a **union type** so functions can accept *any valid model name* without losing IntelliSense/type hints.\n",
    "* **`ServiceMetadata`** is a **Pydantic schema** for storing and validating what’s available in your AI service setup.\n",
    "\n",
    "If you were to load this from a config or an API, you could do:\n",
    "\n",
    "```python\n",
    "service_meta = ServiceMetadata(\n",
    "    agents=[AgentInfo(name=\"research-assistant\")],\n",
    "    models=[OpenAIModelName.GPT_4O, AnthropicModelName.SONNET_35],\n",
    "    default_agent=\"research-assistant\",\n",
    "    default_model=OpenAIModelName.GPT_4O\n",
    ")\n",
    "```\n",
    "\n",
    "…and Pydantic would enforce that you don’t accidentally type `\"gpt4o\"` or `\"Claude_Sonnet\"` incorrectly.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can **diagram how all these enums and the Pydantic model connect** so you can see the relationship visually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a060995-718a-433f-b3e9-b0cd8f4b502e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef5fffef-214e-4b4d-8bc1-ffc17a90ff3c",
   "metadata": {},
   "source": [
    "Great questions — you’ve bumped into the “Enums are objects but can also be strings” wrinkle. Let’s untangle it.\n",
    "\n",
    "### What is `AllModelEnum`, really?\n",
    "\n",
    "`AllModelEnum` is a **type alias** for a **union of several `StrEnum` classes** (each class holds model names for one provider). So a value of type `AllModelEnum` is **an enum member** from *one of those classes*, e.g. `OpenAIModelName.GPT_4O` or `AnthropicModelName.SONNET_35`.\n",
    "\n",
    "### Are these “strings” or “objects”?\n",
    "\n",
    "Both, kind of:\n",
    "\n",
    "* Each member (e.g. `OpenAIModelName.GPT_4O`) is an **enum object**.\n",
    "* Because they inherit from **`StrEnum`**, they also **subclass `str`**. That means they behave like strings and **serialize as strings**, but they still carry their enum type.\n",
    "\n",
    "```py\n",
    "from enum import StrEnum\n",
    "\n",
    "class OpenAIModelName(StrEnum):\n",
    "    GPT_4O = \"gpt-4o\"\n",
    "\n",
    "m = OpenAIModelName.GPT_4O\n",
    "assert isinstance(m, OpenAIModelName)  # True (enum member)\n",
    "assert isinstance(m, str)              # True (because StrEnum)\n",
    "m == \"gpt-4o\"                          # True\n",
    "m.value                                # \"gpt-4o\"\n",
    "```\n",
    "\n",
    "### So is the `Field` description wrong?\n",
    "\n",
    "No. This line:\n",
    "\n",
    "```python\n",
    "default_model: AllModelEnum = Field(\n",
    "    description=\"Default model used when none is specified.\",\n",
    ")\n",
    "```\n",
    "\n",
    "…means **the field holds an enum member** from any of the listed `StrEnum`s (the union). It is **not** typed as a plain `str`, but:\n",
    "\n",
    "* **Pydantic will accept a string** like `\"gpt-4o\"` and coerce it into the correct enum member.\n",
    "* **When exporting to JSON**, Pydantic will emit the **string value** (e.g. `\"gpt-4o\"`).\n",
    "\n",
    "So: *type-wise*, it’s an enum member; *at runtime and on the wire*, it behaves like (and becomes) a string.\n",
    "\n",
    "### “Providers” vs “Models”\n",
    "\n",
    "* `Provider` is a separate `StrEnum` listing providers (OpenAI, Anthropic, etc.). It uses `auto()`, so each member’s value is its **name as a string** (e.g. `\"OPENAI\"`).\n",
    "* `AllModelEnum` does **not** include `Provider`. It’s only about **model names** (like `\"gpt-4o\"`, `\"claude-3.5-sonnet\"`, etc.), grouped per provider.\n",
    "\n",
    "### One important caveat (you do have it!)\n",
    "\n",
    "You’ve got **duplicate string values** across different enums (e.g. `gemini-1.5-pro` exists in both `GoogleModelName` and `VertexAIModelName`, with slightly different prefixes in one case). When you validate a **Union of Enums**, Pydantic picks the **first matching enum** in the union order. That can be surprising:\n",
    "\n",
    "```py\n",
    "# If both enums contain \"gemini-1.5-pro\"\n",
    "ServiceMetadata(default_model=\"gemini-1.5-pro\")\n",
    "# will be parsed as whichever enum appears first in AllModelEnum.\n",
    "```\n",
    "\n",
    "If you want to avoid ambiguity, consider one of these designs:\n",
    "\n",
    "1. **Single flat `StrEnum`** with all unique model values.\n",
    "2. **Structured ID**: store `{ provider: Provider, model: str }` (or `{provider, model_enum}`) so collisions can’t happen.\n",
    "3. **Namespace the values** (e.g. `\"google/gemini-1.5-pro\"` vs `\"vertexai/gemini-1.5-pro\"`).\n",
    "\n",
    "### TL;DR\n",
    "\n",
    "* `default_model` is typed as **an enum member** (not plain `str`), but it **behaves like a string** and **serializes to a string** because it’s a `StrEnum`.\n",
    "* `Provider` is unrelated to `AllModelEnum` and simply lists providers.\n",
    "* Your description is fine; just be aware of **value collisions** across different model enums in the union.\n",
    "\n",
    "If you want, I can show a quick tweak to make your schema unambiguous (e.g., a tiny `ModelRef` Pydantic model with `{provider, name}`) and how to migrate your current code in a couple of lines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e09ae38-2c8d-469a-91f9-8485553b8429",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1401fdbf-6403-4d59-a7bd-575863cdfe6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d52a626b-d6cb-48ff-936a-de3c485044b6",
   "metadata": {},
   "source": [
    "# @router.post(\"/{agent_id}/invoke\")\n",
    "# @router.post(\"/invoke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb4f8bb-eaf0-49fb-ab37-ee3e9021008b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# async def _handle_input(user_input: UserInput, agent: AgentGraph) -> tuple[dict[str, Any], UUID]:\n",
    "#     \"\"\"\n",
    "#     Parse user input and handle any required interrupt resumption.\n",
    "#     Returns kwargs for agent invocation and the run_id.\n",
    "#     \"\"\"\n",
    "#     run_id = uuid4()\n",
    "#     thread_id = user_input.thread_id or str(uuid4())\n",
    "#     user_id = user_input.user_id or str(uuid4())\n",
    "\n",
    "#     configurable = {\"thread_id\": thread_id, \"model\": user_input.model, \"user_id\": user_id}\n",
    "\n",
    "#     callbacks = []\n",
    "#     if settings.LANGFUSE_TRACING:\n",
    "#         # Initialize Langfuse CallbackHandler for Langchain (tracing)\n",
    "#         langfuse_handler = CallbackHandler()\n",
    "\n",
    "#         callbacks.append(langfuse_handler)\n",
    "\n",
    "#     if user_input.agent_config:\n",
    "#         if overlap := configurable.keys() & user_input.agent_config.keys():\n",
    "#             raise HTTPException(\n",
    "#                 status_code=422,\n",
    "#                 detail=f\"agent_config contains reserved keys: {overlap}\",\n",
    "#             )\n",
    "#         configurable.update(user_input.agent_config)\n",
    "\n",
    "#     config = RunnableConfig(\n",
    "#         configurable=configurable,\n",
    "#         run_id=run_id,\n",
    "#         callbacks=callbacks,\n",
    "#     )\n",
    "\n",
    "#     # Check for interrupts that need to be resumed\n",
    "#     state = await agent.aget_state(config=config)\n",
    "#     interrupted_tasks = [\n",
    "#         task for task in state.tasks if hasattr(task, \"interrupts\") and task.interrupts\n",
    "#     ]\n",
    "\n",
    "#     input: Command | dict[str, Any]\n",
    "#     if interrupted_tasks:\n",
    "#         # assume user input is response to resume agent execution from interrupt\n",
    "#         input = Command(resume=user_input.message)\n",
    "#     else:\n",
    "#         input = {\"messages\": [HumanMessage(content=user_input.message)]}\n",
    "\n",
    "#     kwargs = {\n",
    "#         \"input\": input,\n",
    "#         \"config\": config,\n",
    "#     }\n",
    "\n",
    "#     return kwargs, run_id\n",
    "\n",
    "\n",
    "# @router.post(\"/{agent_id}/invoke\")\n",
    "# @router.post(\"/invoke\")\n",
    "# async def invoke(user_input: UserInput, agent_id: str = DEFAULT_AGENT) -> ChatMessage:\n",
    "#     \"\"\"\n",
    "#     Invoke an agent with user input to retrieve a final response.\n",
    "\n",
    "#     If agent_id is not provided, the default agent will be used.\n",
    "#     Use thread_id to persist and continue a multi-turn conversation. run_id kwarg\n",
    "#     is also attached to messages for recording feedback.\n",
    "#     Use user_id to persist and continue a conversation across multiple threads.\n",
    "#     \"\"\"\n",
    "#     # NOTE: Currently this only returns the last message or interrupt.\n",
    "#     # In the case of an agent outputting multiple AIMessages (such as the background step\n",
    "#     # in interrupt-agent, or a tool step in research-assistant), it's omitted. Arguably,\n",
    "#     # you'd want to include it. You could update the API to return a list of ChatMessages\n",
    "#     # in that case.\n",
    "#     agent: AgentGraph = get_agent(agent_id)\n",
    "#     kwargs, run_id = await _handle_input(user_input, agent)\n",
    "\n",
    "#     try:\n",
    "#         response_events: list[tuple[str, Any]] = await agent.ainvoke(**kwargs, stream_mode=[\"updates\", \"values\"])  # type: ignore # fmt: skip\n",
    "#         response_type, response = response_events[-1]\n",
    "#         if response_type == \"values\":\n",
    "#             # Normal response, the agent completed successfully\n",
    "#             output = langchain_to_chat_message(response[\"messages\"][-1])\n",
    "#         elif response_type == \"updates\" and \"__interrupt__\" in response:\n",
    "#             # The last thing to occur was an interrupt\n",
    "#             # Return the value of the first interrupt as an AIMessage\n",
    "#             output = langchain_to_chat_message(\n",
    "#                 AIMessage(content=response[\"__interrupt__\"][0].value)\n",
    "#             )\n",
    "#         else:\n",
    "#             raise ValueError(f\"Unexpected response type: {response_type}\")\n",
    "\n",
    "#         output.run_id = str(run_id)\n",
    "#         return output\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"An exception occurred: {e}\")\n",
    "#         raise HTTPException(status_code=500, detail=\"Unexpected error\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d1afad-aebd-4659-a947-44dbd49dd2b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4375352-e9d6-41b6-b790-0a506d139a8f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c1b46c9-201e-4f01-b8d3-65fac0f545c7",
   "metadata": {},
   "source": [
    "A concise summary of what happens in `_handle_input(...)` and the two routes `/invoke` and `/stream`:\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 `_handle_input(user_input, agent)`\n",
    "\n",
    "* Generates a unique `run_id`, and fills in `thread_id` and `user_id` from input (or random UUIDs).\n",
    "* Builds a `configurable` dict with `{thread_id, user_id, model}`, and merges `agent_config` if provided (ensuring no key conflicts).\n",
    "* Optionally adds a **Langfuse callback** if tracing is enabled.\n",
    "* Constructs a `RunnableConfig` with the above data.\n",
    "* Checks agent’s state to see if it's **awaiting input (interrupted)**.\n",
    "\n",
    "  * If yes → wraps user message in `Command(resume=...)`\n",
    "  * If no → wraps it as a `HumanMessage(...)`\n",
    "* Returns a tuple of `{input, config}` and `run_id`.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 `POST /invoke`\n",
    "\n",
    "* Gets the agent (by ID or default).\n",
    "* Calls `_handle_input(...)` to prepare config + input.\n",
    "* Runs the agent using `agent.ainvoke(...)` with `stream_mode=[\"updates\", \"values\"]`.\n",
    "* Gets the **last event** in the response:\n",
    "\n",
    "  * If `\"values\"` → extracts last message and returns it.\n",
    "  * If `\"updates\"` with `\"__interrupt__\"` → returns first interrupt as an AI message.\n",
    "* Converts the result into a `ChatMessage`, attaches `run_id`, and returns it.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 `POST /stream`\n",
    "\n",
    "* Also calls `_handle_input(...)`.\n",
    "* Streams agent output using `agent.astream(...)` (async generator).\n",
    "* Handles:\n",
    "\n",
    "  * `updates`: intermediate messages and interrupts\n",
    "  * `messages`: token-by-token chunks (optional)\n",
    "  * `custom`: metadata messages\n",
    "* Converts stream events to `ChatMessage` or tokens, emits as **SSE (`data: {...}`)**.\n",
    "* Skips echoed human input and tool-use chunks.\n",
    "* Ends with `data: [DONE]`.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "![image](images/handle_input_and_two_routes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be739be-ae97-4aed-9af3-d9ccffc8f14d",
   "metadata": {},
   "source": [
    "\n",
    "## 🔹 PART 1: `_handle_input(...)`\n",
    "\n",
    "This function **prepares the input and config** for the agent.\n",
    "\n",
    "```python\n",
    "async def _handle_input(user_input: UserInput, agent: AgentGraph) -> tuple[dict[str, Any], UUID]:\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 What it does — at a high level:\n",
    "\n",
    "* Generates unique IDs for tracking\n",
    "* Builds the configuration for the LangChain agent (`RunnableConfig`)\n",
    "* Sets up **Langfuse** tracing (optional)\n",
    "* Handles **agent interruptions** (resuming an agent mid-task)\n",
    "* Returns:\n",
    "\n",
    "  * `kwargs` → to be passed into `agent.ainvoke(...)`\n",
    "  * `run_id` → for tracking this request\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 Line-by-line breakdown\n",
    "\n",
    "---\n",
    "\n",
    "#### `run_id = uuid4()`\n",
    "\n",
    "* A new unique run ID — used to identify this specific conversation turn\n",
    "* Later attached to responses for feedback or tracing\n",
    "\n",
    "---\n",
    "\n",
    "#### `thread_id = user_input.thread_id or str(uuid4())`\n",
    "\n",
    "* Reuses provided thread ID if available; otherwise creates a new one\n",
    "* A thread represents one conversation (e.g. a chat session)\n",
    "\n",
    "---\n",
    "\n",
    "#### `user_id = user_input.user_id or str(uuid4())`\n",
    "\n",
    "* Same idea: each user gets a unique identifier unless one is already provided\n",
    "\n",
    "---\n",
    "\n",
    "#### `configurable = {\"thread_id\": ..., \"model\": ..., \"user_id\": ...}`\n",
    "\n",
    "* This dictionary will be used in `RunnableConfig`\n",
    "* These keys are **reserved** and must not be overwritten by user-supplied `agent_config`\n",
    "\n",
    "---\n",
    "\n",
    "#### `callbacks = []`\n",
    "\n",
    "* A list of LangChain callbacks (observers)\n",
    "* Will include a Langfuse callback if tracing is enabled\n",
    "\n",
    "---\n",
    "\n",
    "#### Langfuse tracing (optional):\n",
    "\n",
    "```python\n",
    "if settings.LANGFUSE_TRACING:\n",
    "    langfuse_handler = CallbackHandler()\n",
    "    callbacks.append(langfuse_handler)\n",
    "```\n",
    "\n",
    "* Hooks into LangChain’s run system to log traces in Langfuse\n",
    "\n",
    "---\n",
    "\n",
    "#### `if user_input.agent_config:`\n",
    "\n",
    "* Users can pass extra config to the agent (e.g., temperature, max\\_tokens)\n",
    "* First, it checks if any keys in `agent_config` **conflict with reserved keys**\n",
    "\n",
    "```python\n",
    "if overlap := configurable.keys() & user_input.agent_config.keys():\n",
    "    raise HTTPException(...)\n",
    "```\n",
    "\n",
    "* If no conflicts, user config is merged in:\n",
    "\n",
    "```python\n",
    "configurable.update(user_input.agent_config)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### `config = RunnableConfig(...)`\n",
    "\n",
    "* Final config object passed to the LangChain agent\n",
    "* Includes:\n",
    "\n",
    "  * `configurable`: all thread/user/model + agent config\n",
    "  * `run_id`: for tracing/feedback\n",
    "  * `callbacks`: observers (Langfuse etc.)\n",
    "\n",
    "---\n",
    "\n",
    "#### Interrupt handling:\n",
    "\n",
    "```python\n",
    "state = await agent.aget_state(config=config)\n",
    "interrupted_tasks = [task for task in state.tasks if hasattr(task, \"interrupts\") and task.interrupts]\n",
    "```\n",
    "\n",
    "* Checks if the agent has any **interrupted tasks** in its task state (e.g., awaiting a user response)\n",
    "* If so, the user’s message is treated as a **resume signal**:\n",
    "\n",
    "```python\n",
    "input = Command(resume=user_input.message)\n",
    "```\n",
    "\n",
    "* Otherwise, it's treated as a **new HumanMessage**:\n",
    "\n",
    "```python\n",
    "input = {\"messages\": [HumanMessage(content=user_input.message)]}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Return value:\n",
    "\n",
    "```python\n",
    "return {\"input\": ..., \"config\": ...}, run_id\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 PART 2: `@router.post(\"/invoke\")`\n",
    "\n",
    "```python\n",
    "async def invoke(user_input: UserInput, agent_id: str = DEFAULT_AGENT) -> ChatMessage:\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 What it does:\n",
    "\n",
    "* Accepts a user input (message, thread ID, user ID, etc.)\n",
    "* Uses `_handle_input(...)` to prepare for execution\n",
    "* Invokes the selected agent via `agent.ainvoke(...)`\n",
    "* Extracts the **final result or interrupt**\n",
    "* Converts it into a `ChatMessage` object\n",
    "* Returns it to the caller\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 Breakdown\n",
    "\n",
    "---\n",
    "\n",
    "#### `agent: AgentGraph = get_agent(agent_id)`\n",
    "\n",
    "* Grabs the `AgentGraph` instance matching the provided ID\n",
    "* This object handles the actual LangGraph execution\n",
    "\n",
    "---\n",
    "\n",
    "#### `kwargs, run_id = await _handle_input(...)`\n",
    "\n",
    "* Prepares the configuration and formatted input\n",
    "* Handles Langfuse, interrupt state, agent\\_config, etc.\n",
    "\n",
    "---\n",
    "\n",
    "#### Main agent execution:\n",
    "\n",
    "```python\n",
    "response_events = await agent.ainvoke(..., stream_mode=[\"updates\", \"values\"])\n",
    "response_type, response = response_events[-1]\n",
    "```\n",
    "\n",
    "* `agent.ainvoke(...)` returns a list of events (a stream, even in non-streaming mode)\n",
    "* The **last** event is assumed to contain the final result\n",
    "\n",
    "---\n",
    "\n",
    "#### Handling different response types:\n",
    "\n",
    "##### ✅ Case 1: Agent completed successfully\n",
    "\n",
    "```python\n",
    "if response_type == \"values\":\n",
    "    output = langchain_to_chat_message(response[\"messages\"][-1])\n",
    "```\n",
    "\n",
    "* Converts the **last message** from LangChain into your custom `ChatMessage` format\n",
    "\n",
    "---\n",
    "\n",
    "##### 🟨 Case 2: Agent was interrupted\n",
    "\n",
    "```python\n",
    "elif response_type == \"updates\" and \"__interrupt__\" in response:\n",
    "    output = langchain_to_chat_message(AIMessage(content=response[\"__interrupt__\"][0].value))\n",
    "```\n",
    "\n",
    "* The agent paused mid-process and requested input from the user\n",
    "* You take the first interrupt's content and return it as an `AIMessage`\n",
    "\n",
    "---\n",
    "\n",
    "##### ❌ Unexpected:\n",
    "\n",
    "```python\n",
    "else:\n",
    "    raise ValueError(...)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Add run\\_id to the message:\n",
    "\n",
    "```python\n",
    "output.run_id = str(run_id)\n",
    "```\n",
    "\n",
    "* This helps the frontend associate the output with the request, and enables **feedback logging** later\n",
    "\n",
    "---\n",
    "\n",
    "#### Final response\n",
    "\n",
    "* Returns a `ChatMessage` (your app’s wrapper class around LangChain messages)\n",
    "* If anything fails, logs the error and returns HTTP 500\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 Summary Flow\n",
    "\n",
    "1. **Frontend sends**: message, model, user ID, thread ID, and agent config (optional)\n",
    "2. **\\_handle\\_input()**:\n",
    "\n",
    "   * Prepares identifiers\n",
    "   * Adds config\n",
    "   * Checks if this is a resume or new input\n",
    "3. **ainvoke()** is called on the agent\n",
    "4. **Last response is returned**:\n",
    "\n",
    "   * If successful → last message\n",
    "   * If interrupted → interrupt message\n",
    "5. Output is wrapped with run\\_id and returned as a `ChatMessage`\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Example Output\n",
    "\n",
    "If the model finishes:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"type\": \"ai\",\n",
    "  \"content\": \"Sure! Here's a summary...\",\n",
    "  \"run_id\": \"70e4a9f7-....\"\n",
    "}\n",
    "```\n",
    "\n",
    "If interrupted:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"type\": \"ai\",\n",
    "  \"content\": \"Can you clarify what you mean by 'this'?\",\n",
    "  \"run_id\": \"70e4a9f7-....\"\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you want to:\n",
    "\n",
    "* Look into `agent.ainvoke(...)` and what it returns\n",
    "* Unpack what `RunnableConfig` does behind the scenes\n",
    "* Dive into `langchain_to_chat_message(...)`\n",
    "* Or trace how interrupts are handled within the LangGraph agent\n",
    "\n",
    "Where to next?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3b8970-a0a5-4887-adf0-c6a8b3218388",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abacb19a-e7fc-4daa-a293-2e0989ac0291",
   "metadata": {},
   "source": [
    "Great — let’s look at this **specific syntax** from a Python syntax point of view (not the functionality, since you already understand what it does).\n",
    "\n",
    "Here’s the block again for context:\n",
    "\n",
    "```python\n",
    "if user_input.agent_config:\n",
    "    if overlap := configurable.keys() & user_input.agent_config.keys():\n",
    "        raise HTTPException(\n",
    "            status_code=422,\n",
    "            detail=f\"agent_config contains reserved keys: {overlap}\",\n",
    "        )\n",
    "    configurable.update(user_input.agent_config)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 1. What does `:=` mean?\n",
    "\n",
    "### It's called the **walrus operator** (introduced in Python 3.8)\n",
    "\n",
    "#### ✅ Purpose:\n",
    "\n",
    "It lets you **assign a value to a variable as part of an expression** — especially useful in `if` or `while` statements.\n",
    "\n",
    "### 📌 Example:\n",
    "\n",
    "```python\n",
    "if (n := len(my_list)) > 10:\n",
    "    print(f\"List is too long! ({n} items)\")\n",
    "```\n",
    "\n",
    "⬆️ Without the walrus operator, you’d have to do:\n",
    "\n",
    "```python\n",
    "n = len(my_list)\n",
    "if n > 10:\n",
    "    print(f\"List is too long! ({n} items)\")\n",
    "```\n",
    "\n",
    "### 🧠 So in your code:\n",
    "\n",
    "```python\n",
    "if overlap := configurable.keys() & user_input.agent_config.keys():\n",
    "```\n",
    "\n",
    "* `overlap` is **assigned** the result of the expression on the right\n",
    "* If it's **truthy** (i.e., the set isn’t empty), the `if` block runs\n",
    "\n",
    "✅ This is equivalent to:\n",
    "\n",
    "```python\n",
    "overlap = configurable.keys() & user_input.agent_config.keys()\n",
    "if overlap:\n",
    "    raise ...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 2. What does `&` mean here?\n",
    "\n",
    "### It's the **set intersection operator**\n",
    "\n",
    "* `&` between two sets returns a new set with elements that are **common to both**\n",
    "\n",
    "#### Example:\n",
    "\n",
    "```python\n",
    "set1 = {\"a\", \"b\", \"c\"}\n",
    "set2 = {\"b\", \"c\", \"d\"}\n",
    "overlap = set1 & set2  # => {\"b\", \"c\"}\n",
    "```\n",
    "\n",
    "🧠 In your code:\n",
    "\n",
    "```python\n",
    "configurable.keys() & user_input.agent_config.keys()\n",
    "```\n",
    "\n",
    "* Both `configurable.keys()` and `agent_config.keys()` return **dict views** that act like sets.\n",
    "* This returns the set of **keys that appear in both**.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Final simplified interpretation:\n",
    "\n",
    "```python\n",
    "if user_input.agent_config:\n",
    "    overlap = set(configurable.keys()) & set(user_input.agent_config.keys())\n",
    "    if overlap:\n",
    "        raise HTTPException(...)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 TL;DR\n",
    "\n",
    "| Symbol | Meaning                                    | Usage in your code                                 |\n",
    "| ------ | ------------------------------------------ | -------------------------------------------------- |\n",
    "| `:=`   | Walrus operator (assign inside expression) | Assigns `overlap` while checking its value in `if` |\n",
    "| `&`    | Set intersection                           | Finds conflicting keys in two dictionaries         |\n",
    "\n",
    "Let me know if you want to practice writing some short examples using `:=` or `&`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611e516d-9730-47be-a6d3-ce42bce82acd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53be17d9-6e18-4ed2-87e5-37ebd10c9da7",
   "metadata": {},
   "source": [
    "Great question — and you’re not alone. This part is a bit obscure, especially if you haven’t worked with `LangChainChatMessage(role=\"custom\", ...)` or custom agents before.\n",
    "\n",
    "Let’s break it down *clearly* and visually, starting with context, what `role == \"custom\"` means, and what’s being returned.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Context\n",
    "\n",
    "This block is inside:\n",
    "\n",
    "```python\n",
    "def langchain_to_chat_message(message: BaseMessage) -> ChatMessage:\n",
    "```\n",
    "\n",
    "That function converts **LangChain message types** into your app’s unified `ChatMessage` format for API responses.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔸 First, what is `LangchainChatMessage`?\n",
    "\n",
    "LangChain defines a flexible message class:\n",
    "\n",
    "```python\n",
    "class ChatMessage(BaseMessage):\n",
    "    role: str  # can be \"system\", \"user\", \"assistant\", \"custom\", etc.\n",
    "    content: Union[str, list[dict[str, Any]]]\n",
    "```\n",
    "\n",
    "This lets you represent arbitrary messages like:\n",
    "\n",
    "```python\n",
    "ChatMessage(role=\"system\", content=\"You are a helpful assistant.\")\n",
    "ChatMessage(role=\"user\", content=\"What's the weather?\")\n",
    "ChatMessage(role=\"custom\", content=[{\"mydata\": \"abc\"}])\n",
    "```\n",
    "\n",
    "It's **role-based**, not type-based like `AIMessage`, `HumanMessage`, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔸 Why `role == \"custom\"` is handled separately\n",
    "\n",
    "LangChain allows developers to define **custom message roles** beyond the defaults (like `\"system\"`, `\"user\"`, `\"assistant\"`). If you're building agents or supervisors that carry their own metadata, you might emit a message like:\n",
    "\n",
    "```python\n",
    "LangchainChatMessage(role=\"custom\", content=[{\"agent_name\": \"researcher\", \"summary\": \"...\"}])\n",
    "```\n",
    "\n",
    "This is usually **not meant to be shown as raw user/AI text**, but as metadata.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 So what's this doing?\n",
    "\n",
    "```python\n",
    "if message.role == \"custom\":\n",
    "    custom_message = ChatMessage(\n",
    "        type=\"custom\",\n",
    "        content=\"\",\n",
    "        custom_data=message.content[0],  # first item in the content list\n",
    "    )\n",
    "    return custom_message\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "* It **ignores the normal `content` field**, assuming it's **structured metadata**\n",
    "* It pulls the **first item** (a `dict`) from the content list\n",
    "* That dict is stored in `custom_data`, not `content`\n",
    "* The `type` is set to `\"custom\"` so your frontend can treat it differently\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Is this a `text` message or a `chat` message?\n",
    "\n",
    "Neither, directly.\n",
    "\n",
    "This is a **custom metadata message**, **not intended as assistant/user dialogue**.\n",
    "\n",
    "### So:\n",
    "\n",
    "* ❌ It's **not** meant to be converted to plain text (`str`)\n",
    "* ✅ It's wrapped as a `ChatMessage` with `type=\"custom\"` and no readable `content`\n",
    "* ✅ Its metadata is available in `custom_data` for the frontend or tools to process\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 What would a full example look like?\n",
    "\n",
    "```python\n",
    "# Raw LangChain message\n",
    "LangchainChatMessage(\n",
    "    role=\"custom\",\n",
    "    content=[{\n",
    "        \"agent_name\": \"supervisor\",\n",
    "        \"task_id\": \"abc123\",\n",
    "        \"summary\": \"Research phase complete.\"\n",
    "    }]\n",
    ")\n",
    "```\n",
    "\n",
    "Would be converted to:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"type\": \"custom\",\n",
    "  \"content\": \"\",\n",
    "  \"custom_data\": {\n",
    "    \"agent_name\": \"supervisor\",\n",
    "    \"task_id\": \"abc123\",\n",
    "    \"summary\": \"Research phase complete.\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "Your frontend could then say:\n",
    "\n",
    "> “The supervisor agent finished a research task.”\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 Summary\n",
    "\n",
    "| Feature                          | Value                                              |\n",
    "| -------------------------------- | -------------------------------------------------- |\n",
    "| Is it an AI/user/system message? | ❌ No                                               |\n",
    "| Is it a text message?            | ❌ No                                               |\n",
    "| What is it?                      | ✅ A special metadata message with custom structure |\n",
    "| How is it returned?              | As a `ChatMessage` with `type=\"custom\"`            |\n",
    "| Where is the data?               | In `custom_data`, extracted from `content[0]`      |\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Key takeaways:\n",
    "\n",
    "* `role=\"custom\"` is a LangChain escape hatch for arbitrary metadata\n",
    "* These messages are **not meant to be rendered like chat** — they're informational\n",
    "* Your app converts them into `ChatMessage(type=\"custom\")` with the first dict inside `custom_data`\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to show:\n",
    "\n",
    "* Where in LangGraph/agents these custom messages may originate?\n",
    "* Or how to safely generalize this to support multiple items, not just `content[0]`?\n",
    "\n",
    "Let me know what you're curious about next.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36618468-60fb-4d09-856e-27dcaebc5026",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e8089b-7127-425d-93d6-52d4e4d36eb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63bd55e4-5b54-47ef-a6ea-7a1ef745342f",
   "metadata": {},
   "source": [
    "## Thinking Mode with whole codebase access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b81cfa7-1f4e-44cb-8544-21eb703fe352",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Below is a line-by-line “exploded view” of the two helpers so you can see:\n",
    "\n",
    "* **where every symbol comes from,**\n",
    "* **why each step exists,** and\n",
    "* **how they fit into LangGraph’s execution model.**\n",
    "\n",
    "---\n",
    "\n",
    "## 1. `_handle_input(...)`\n",
    "\n",
    "| Step | Code                                                                                                     | Purpose / Notes                                                                                                                                                                              | Origin of types                                                   |          |\n",
    "| ---- | -------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------- | -------- |\n",
    "| 1    | `run_id = uuid4()`                                                                                       | Generates a unique ID for *this* call. The ID is later copied onto every outgoing `ChatMessage` so the front-end can send feedback to the right run.                                         | stdlib `uuid4()`                                                  |          |\n",
    "| 2    | `thread_id = user_input.thread_id or str(uuid4())`<br>`user_id   = user_input.user_id   or str(uuid4())` | Guarantees a **thread scope** (conversation) and a **user scope** (cross-thread identity) even if the client doesn’t supply them.                                                            | `schema.UserInput`                                                |          |\n",
    "| 3    | `configurable = {\"thread_id\": …, \"model\": user_input.model, \"user_id\": …}`                               | LangGraph lets you pass arbitrary **configurable** keys through `RunnableConfig`; they’re available to every node via `config.get(\"configurable\")`. The toolkit standardises on these three. | LangGraph `RunnableConfig`                                        |          |\n",
    "| 4    | *Langfuse tracing*                                                                                       | If `settings.LANGFUSE_TRACING` is on, a `CallbackHandler` from **Langfuse** is appended so every node is traced automatically.                                                               | `langfuse.CallbackHandler`                                        |          |\n",
    "| 5    | *Merge* `user_input.agent_config`                                                                        | The REST API lets callers add extra model parameters (e.g. `{\"temperature\":0.3}`). Collision with the reserved keys above triggers `HTTP 422`.                                               | user supplied                                                     |          |\n",
    "| 6    | `config = RunnableConfig( … )`                                                                           | Bundles<br>• `run_id` (needed so feedback goes to the right trace)<br>• `callbacks` (Langfuse list)<br>• `configurable` (dict)                                                               | LangGraph core                                                    |          |\n",
    "| 7    | `state = await agent.aget_state(config=config)`                                                          | Pulls the *latest saved* state for this thread from the **checkpointer** that was injected in `lifespan()`.                                                                                  | \\`AgentGraph = CompiledStateGraph                                 | Pregel\\` |\n",
    "| 8    | `interrupted_tasks = [t for t in state.tasks if t.interrupts]`                                           | Looks for any node that raised `Interrupt`. That’s how LangGraph signals “pause and wait for the human”.                                                                                     | LangGraph `Interrupt`                                             |          |\n",
    "| 9    | `input = Command(resume=...)` **or** `{\"messages\":[HumanMessage(...) ]}`                                 | • If we’re resuming, wrap the human’s reply in `Command(resume=…)`.<br>• Otherwise provide a normal list of `HumanMessage`s.                                                                 | `langgraph.types.Command`, `langchain_core.messages.HumanMessage` |          |\n",
    "| 10   | `return {\"input\": input, \"config\": config}, run_id`                                                      | These kwargs are exactly what `agent.ainvoke()` / `agent.astream()` expect.                                                                                                                  | —                                                                 |          |\n",
    "\n",
    "---\n",
    "\n",
    "### Why the `Command(resume=…)` dance?\n",
    "\n",
    "LangGraph v0.3 introduced `interrupt()`—a node can `raise Interrupt(…)` and the graph pauses. To *resume* you call the graph again with `Command(resume=<human-reply>)`; LangGraph rewinds to the interrupted node and continues. `_handle_input` detects that pattern so the REST layer can stay totally stateless.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. `invoke(...)`\n",
    "\n",
    "```python\n",
    "agent: AgentGraph = get_agent(agent_id)\n",
    "kwargs, run_id   = await _handle_input(...)\n",
    "response_events  = await agent.ainvoke(**kwargs, stream_mode=[\"updates\", \"values\"])\n",
    "```\n",
    "\n",
    "| Phase                   | Detail                                                                                                                                                                                                              |\n",
    "| ----------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Dispatch**            | `get_agent()` returns the compiled graph (or a Pregel function) registered under that name.                                                                                                                         |\n",
    "| **Execution**           | We call `.ainvoke()` **once** and ask LangGraph to give us two stream channels:<br>• `\"updates\"` – intermediate node payloads (used for interrupt detection)<br>• `\"values\"`  – the final return value of the graph |\n",
    "| **Pick the last event** | LangGraph yields a list like `[( \"updates\", {...} ), ( \"values\", {\"messages\":[...]} )]`. We only care about the tail.                                                                                               |\n",
    "| **Normal completion**   | If the tail is `\"values\"` → convert the last message (`AIMessage`) to your public `ChatMessage` DTO.                                                                                                                |\n",
    "| **Interrupted**         | If the tail is `\"updates\"` and the dict contains `\"__interrupt__\"` → take the *first* interrupt object, wrap it in an `AIMessage`.                                                                                  |\n",
    "| **Attach run\\_id**      | `output.run_id = str(run_id)` so any `/feedback` call can map stars → run.                                                                                                                                          |\n",
    "| **Return**              | A single `ChatMessage` (type `\"ai\"`) is sent back.                                                                                                                                                                  |\n",
    "\n",
    "> **⚠️ Note**\n",
    "> If a graph emits *several* AI messages before finishing (e.g., tool-call patterns), only the final one is returned. The comment in the code points this out—you could change the REST contract to return a list if you need the full chain.\n",
    "\n",
    "---\n",
    "\n",
    "## External pieces you might want to open next\n",
    "\n",
    "| Symbol                        | Defined in             | Why it matters                                                                       |\n",
    "| ----------------------------- | ---------------------- | ------------------------------------------------------------------------------------ |\n",
    "| `UserInput`, `ChatMessage`    | `src/schema/schema.py` | Pydantic models for request/response.                                                |\n",
    "| `AgentGraph`, `get_agent()`   | `src/agents/agents.py` | Registry + type alias for compiled LangGraph graphs.                                 |\n",
    "| `RunnableConfig`              | `langgraph.runnables`  | Carries `run_id`, callbacks, and your custom `configurable` dict through every node. |\n",
    "| `Command`, `Interrupt`        | `langgraph.types`      | Mechanism for pause/resume.                                                          |\n",
    "| `langchain_to_chat_message()` | `src/service/utils.py` | Converts LangChain message objects → your façade model.                              |\n",
    "\n",
    "Just let me know which of those (or any other) you’d like to dive into—whether that’s viewing the raw source, walking through control-flow, or discussing how to extend/modify it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ae5131-b8c4-47d1-9f18-27923e44d040",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c869e51e-4913-4b1b-86b6-8bd733ca5586",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa7061a-f7ed-4262-a5c7-e1cbd80fece5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2701d41b-01d5-4868-8a6f-d46852f904b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eba04532-a690-4331-b279-6c20fdcfb4a6",
   "metadata": {},
   "source": [
    "## What Is a LangGraph Supervisor?\n",
    "\n",
    "**LangGraph Supervisor** is a simple Python library created by the LangChain team to help developers build **hierarchical multi-agent systems**—that is, systems where a **central \"supervisor\" agent** coordinates several **specialist \"worker\" agents** ([LangChain Changelog][1]).\n",
    "\n",
    "### How It Works — Explained Simply:\n",
    "\n",
    "* **Supervisor Agent (like a manager):** Acts as the brain of the system. It reads what the user wants and decides which worker agent should do the job—whether that’s researching information, doing math, or something else.\n",
    "* **Worker Agents (like specialists):** Each one focuses on a specific task. For example, one might be a math expert, another a research assistant. They do the work and then report back to the supervisor ([GitHub][2]).\n",
    "\n",
    "### Why It’s Useful:\n",
    "\n",
    "* **Keeps things organized.** The supervisor controls which agent works when and keeps all communication in order.\n",
    "* **Flexible structure.** You can build multiple levels of supervisors—one supervisor can manage others—creating complex, layered workflows ([GitHub][2], [LangChain Changelog][1]).\n",
    "* **Efficient memory handling.** You can choose whether to store the full conversation history or just the last message from each agent ([GitHub][2]).\n",
    "\n",
    "---\n",
    "\n",
    "### Real-World Analogy:\n",
    "\n",
    "Imagine a **restaurant**:\n",
    "\n",
    "* The **head chef** (supervisor) delegates tasks—“Sous-chef, prep the vegetables”; “Grill-chef, cook the steak”.\n",
    "* Each cook (worker agent) does their part and then lets the head chef know they’re done.\n",
    "* The head chef then plates the dish and serves the customer.\n",
    "\n",
    "That’s exactly how a LangGraph Supervisor works in an AI system. It delegates, coordinates, and manages communication so that tasks get done in an orderly way.\n",
    "\n",
    "---\n",
    "\n",
    "### In Summary:\n",
    "\n",
    "* **Supervisor = Manager**: Decides who does what.\n",
    "* **Workers = Specialists**: Do the actual tasks.\n",
    "* **Hierarchy possible**: Supervisors can manage other supervisors.\n",
    "* **Flexible memory**: You choose how much of the conversation to keep.\n",
    "\n",
    "---\n",
    "\n",
    "https://changelog.langchain.com/announcements/langgraph-supervisor-a-library-for-hierarchical-multi-agent-systems?utm_source=chatgpt.com \n",
    "\n",
    "\"LangGraph Supervisor: A Library for\"\n",
    "\n",
    "https://github.com/langchain-ai/langgraph-supervisor-py?utm_source=chatgpt.com \"langchain-ai/langgraph-supervisor-py\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b21a37-1c68-439d-a56a-7fef518201bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23399300-1497-451d-b894-c96dbbcc696a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb96f29-4672-47a6-82ad-328a2b8b53cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434f5f01-d519-4966-b0e3-4c3d083f2785",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf24465-457b-425a-8cee-4f94bf00a9e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf14ecc3-3d01-4fea-9be6-0f7918f1c5f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a95545d-3fff-46ee-bc30-11aa3cf9c936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f372d2-727f-4d73-a5c9-06d68f63ad71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7394c69-4e66-430f-a585-cf45d913e23a",
   "metadata": {},
   "source": [
    "## some prompts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e850d3-be7f-4470-a5f4-fc6a2408b1a0",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "current file's folder:\n",
    "C:\\Users\\Ans\\Desktop\\code\\16_agent_services_toolkit\\agent-service-toolkit\\src\\service\n",
    "\n",
    "it's contents:\n",
    ".ipynb_checkpoints\n",
    "explanantions.ipynb\n",
    "service.py\n",
    "utils.py\n",
    "__init__.py\n",
    "\n",
    "ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo\n",
    "\n",
    "parent folder address:\n",
    "C:\\Users\\Ans\\Desktop\\code\\16_agent_services_toolkit\\agent-service-toolkit\\src\n",
    "contents:\n",
    "agents\n",
    "client\n",
    "core\n",
    "memory\n",
    "run_agent.py\n",
    "run_client.py\n",
    "run_service.py\n",
    "schema\n",
    "service\n",
    "streamlit_app.py\n",
    "\n",
    "ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo\n",
    "\n",
    "memmory folder path:\n",
    "C:\\Users\\Ans\\Desktop\\code\\16_agent_services_toolkit\\agent-service-toolkit\\src\\memory\n",
    "contents:\n",
    "mongodb.py\n",
    "postgres.py\n",
    "sqlite.py\n",
    "__init__.py\n",
    "\n",
    "ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo\n",
    "\n",
    "C:\\Users\\Ans\\Desktop\\code\\16_agent_services_toolkit\\agent-service-toolkit\\src\\memory\\__init__.py\n",
    "contents:\n",
    "from contextlib import AbstractAsyncContextManager\n",
    "\n",
    "from langgraph.checkpoint.mongodb.aio import AsyncMongoDBSaver\n",
    "from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver\n",
    "from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver\n",
    "\n",
    "from core.settings import DatabaseType, settings\n",
    "from memory.mongodb import get_mongo_saver\n",
    "from memory.postgres import get_postgres_saver, get_postgres_store\n",
    "from memory.sqlite import get_sqlite_saver, get_sqlite_store\n",
    "\n",
    "\n",
    "def initialize_database() -> AbstractAsyncContextManager[\n",
    "    AsyncSqliteSaver | AsyncPostgresSaver | AsyncMongoDBSaver\n",
    "]:\n",
    "    \"\"\"\n",
    "    Initialize the appropriate database checkpointer based on configuration.\n",
    "    Returns an initialized AsyncCheckpointer instance.\n",
    "    \"\"\"\n",
    "    if settings.DATABASE_TYPE == DatabaseType.POSTGRES:\n",
    "        return get_postgres_saver()\n",
    "    if settings.DATABASE_TYPE == DatabaseType.MONGO:\n",
    "        return get_mongo_saver()\n",
    "    else:  # Default to SQLite\n",
    "        return get_sqlite_saver()\n",
    "\n",
    "\n",
    "def initialize_store():\n",
    "    \"\"\"\n",
    "    Initialize the appropriate store based on configuration.\n",
    "    Returns an async context manager for the initialized store.\n",
    "    \"\"\"\n",
    "    if settings.DATABASE_TYPE == DatabaseType.POSTGRES:\n",
    "        return get_postgres_store()\n",
    "    # TODO: Add Mongo store - https://pypi.org/project/langgraph-store-mongodb/\n",
    "    else:  # Default to SQLite\n",
    "        return get_sqlite_store()\n",
    "\n",
    "\n",
    "__all__ = [\"initialize_database\", \"initialize_store\"]\n",
    "\n",
    "ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo\n",
    "\n",
    "how is it working? memory module was supposed to be in the same folder as the service.py file?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42deb2e-f710-4ece-bff8-1621a2edc08d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "332caad1-1dd0-4aae-8b4d-c9c95bafd738",
   "metadata": {},
   "source": [
    "# authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "567acb97-012c-48b2-bd6c-292448247d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "{'sub': '1234567890'}\n"
     ]
    }
   ],
   "source": [
    "import jwt\n",
    "import base64\n",
    "\n",
    "from sqlmodel import SQLModel\n",
    "\n",
    "class TokenPayload(SQLModel):\n",
    "    sub: str | None = None\n",
    "\n",
    "\n",
    "# Access API keys and credentials\n",
    "AUTH_SECRET : str\n",
    "ALGORITHM   : str\n",
    "\n",
    "\n",
    "# will later get them from core.settings and will put in core.settings from .env\n",
    "#AUTH_SECRET = 'abc123'\n",
    "#AUTH_SECRET = 'Ma9m1zmnlDcHY0XBgozXR5g4bP16mcYRnOgHXjzsLMw='\n",
    "ALGORITHM   = 'HS256'\n",
    "\n",
    "# same Base64 value you used in Node\n",
    "AUTH_SECRET = \"Ma9m1zmnlDcHY0XBgozXR5g4bP16mcYRnOgHXjzsLMw=\"\n",
    "\n",
    "# 1) convert Base64 -> raw bytes\n",
    "AUTH_SECRET_BYTES = base64.b64decode(SECRET_B64.strip())\n",
    "\n",
    "\n",
    "\n",
    "#token = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE3NTcxNzU0NjIsInN1YiI6ImhlbGxvIGJpdGNoZXMhIn0.s4b6hqYPwAH89lFc4Ns2YXhrNPy4xJ4M2HjAsGJ58f4'\n",
    "token = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyb2xlIjoidXNlciIsImlhdCI6MTc1NzE2NTM0MSwiaXNzIjoiaHR0cHM6Ly9leGFtcGxlLmNvbSIsImF1ZCI6Im15LWF1ZGllbmNlIiwic3ViIjoiMTIzNDU2Nzg5MCIsImV4cCI6MTc1NzQyNDU0MX0.WnwIsuMeF2Sf4t3ZbP_yzR6ZeeiSrwV6XwlgnQOGZgM'\n",
    "# Validate a token created by the external issuer (shared secret HS256 in this demo).\n",
    "\n",
    "print('1')\n",
    "\n",
    "payload    = jwt.decode(token,\n",
    "                        AUTH_SECRET_BYTES,\n",
    "                        algorithms=[ALGORITHM],        #) # settings.AUTH_SECRET.get_secret_value()\n",
    "                        issuer=\"https://example.com\",\n",
    "                        audience=\"my-audience\",\n",
    ")\n",
    "                      \n",
    " \n",
    "print('2')\n",
    "token_data = TokenPayload(**payload)\n",
    "print('3')\n",
    "print(token_data.model_dump())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f48b6fe-871b-43b3-a473-e32495ca4659",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb61df1-d4af-4e40-a898-fe6b5ea873ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dfbaeb-18a6-4e5f-96ba-056da416f8ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e30d6c5-b089-4e28-a5c8-9ab11eaef112",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9c1b70-c3e1-4081-a5d6-f9a0f3825ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69da839-0200-42fd-aa6b-d827a2cdc18a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f9996b-1458-43f8-b00a-60b3f30f6388",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b014b06f-db8e-4a72-b19f-49d2d55e5598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c5d5d7-7046-4014-8856-21077d5bb7ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11a8c9e-3049-4ae6-b707-d249a1ef6999",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfea632-1ec2-4656-990e-b61737ec7829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd2561e-7391-4236-bc53-07a3207e7450",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e239daab-6e51-4af3-bb89-28f496101faf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858e8720-1fa2-4138-8771-e5539ed05413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67855a61-2420-4a2f-886a-92a71ea03a02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f508aa01-abb1-4ad0-8acd-929abffeaa6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
